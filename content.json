{"meta":{"title":"lizhewei'Blog","subtitle":"","description":"专注于云原生领域","author":"lizhewei","url":"http://lizhewei91.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-11-25T04:13:14.000Z","updated":"2022-11-25T04:15:44.664Z","comments":true,"path":"categories/index.html","permalink":"http://lizhewei91.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-11-25T04:12:57.000Z","updated":"2022-11-25T04:15:16.176Z","comments":true,"path":"tags/index.html","permalink":"http://lizhewei91.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-06-01T08:32:13.802Z","updated":"2023-06-01T06:31:21.883Z","comments":false,"path":"repository/index.html","permalink":"http://lizhewei91.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"kubeadm 搭建k8s集群-v1.26.3","slug":"deploy-k8s-cluster","date":"2023-03-22T03:58:10.000Z","updated":"2023-03-22T08:34:32.106Z","comments":true,"path":"2023/03/22/10/","link":"","permalink":"http://lizhewei91.github.io/2023/03/22/10/","excerpt":"","text":"","categories":[{"name":"deploy-k8s","slug":"deploy-k8s","permalink":"http://lizhewei91.github.io/categories/deploy-k8s/"}],"tags":[{"name":"kubeadm","slug":"kubeadm","permalink":"http://lizhewei91.github.io/tags/kubeadm/"},{"name":"deploy-k8s","slug":"deploy-k8s","permalink":"http://lizhewei91.github.io/tags/deploy-k8s/"}]},{"title":"kubelet 资源配置","slug":"kubelet-resource-allocation","date":"2023-03-10T07:03:19.000Z","updated":"2023-03-10T08:22:33.164Z","comments":true,"path":"2023/03/10/19/","link":"","permalink":"http://lizhewei91.github.io/2023/03/10/19/","excerpt":"","text":"Kubernetes 的节点可以按照 Capacity 调度。默认情况下 pod 能够使用节点全部可用容量。 这是个问题，因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程。 除非为这些系统守护进程留出资源，否则它们将与 Pod 争夺资源并导致节点资源短缺问题。 kubelet 公开了一个名为 ‘Node Allocatable’ 的特性，有助于为系统守护进程预留计算资源。 Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 ‘Node Allocatable’。 节点可分配资源Kubernetes 节点上的 ‘Allocatable’ 被定义为 Pod 可用计算资源量。 调度器不会超额申请 ‘Allocatable’。 目前支持 **‘CPU’、’memory’ 和 ‘ephemeral-storage’ ** 这几个参数。 我们可以通过 kubectl describe node 命令查看节点可分配资源的数据： $ kubectl describe node node-2...Capacity: cpu: 40 ephemeral-storage: 276590640Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 131273628Ki pods: 255Allocatable: cpu: 38976m ephemeral-storage: 254905933402 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 131171228Ki pods: 255... 可以看到其中有 Capacity 与 Allocatable 两项内容，其中的 Allocatable 就是节点可被分配的资源，我们这里没有配置资源预留，所以默认情况下 Capacity 与 Allocatable 的值基本上是一致的。下图显示了可分配资源和资源预留之间的关系： 目前支持 cpu, memory, ephemeral-storage 三种资源预留。 Node Capacity 是节点的所有硬件资源，kube-reserved 是给 kube 组件预留的资源，system-reserved 是给系统进程预留的资源，eviction-threshold 是 kubelet 驱逐的阈值设定，allocatable 才是真正调度器调度 Pod 时的参考值（保证节点上所有 Pods 的 request 资源不超过 Allocatable）。 节点可分配资源的计算方式为： Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 配置资源预留配置 cgroup 驱动kubelet 支持在主机上使用 cgroup 驱动操作 cgroup 层次结构。 该驱动通过 --cgroup-driver 标志进行配置。 支持的参数值如下： cgroupfs 是默认的驱动，在主机上直接操作 cgroup 文件系统以对 cgroup 沙箱进行管理。 systemd 是可选的驱动，使用 init 系统支持的资源的瞬时切片管理 cgroup 沙箱。 取决于相关容器运行时的配置，操作员可能需要选择一个特定的 cgroup 驱动来保证系统正常运行。 如果操作员使用 containerd 运行时提供的 systemd cgroup 驱动时， 必须配置 kubelet 使用 systemd cgroup 驱动 Kube 预留值 Kubelet 标志：--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000] Kubelet 标志：--kube-reserved-cgroup= kube-reserved 用来给诸如 kubelet、容器运行时、节点问题监测器等 Kubernetes 系统守护进程记述其资源预留值。 该配置并非用来给以 Pod 形式运行的系统守护进程预留资源。kube-reserved 通常是节点上 Pod 密度 的函数。 除了 cpu、内存 和 ephemeral-storage 之外，pid 可用来指定为 Kubernetes 系统守护进程预留指定数量的进程 ID。","categories":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"}],"tags":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"},{"name":"allocatable","slug":"allocatable","permalink":"http://lizhewei91.github.io/tags/allocatable/"}]},{"title":"kubelet 创建pod源码分析","slug":"kubelet-create-pod","date":"2023-03-09T02:33:08.000Z","updated":"2023-03-20T08:26:04.970Z","comments":true,"path":"2023/03/09/08/","link":"","permalink":"http://lizhewei91.github.io/2023/03/09/08/","excerpt":"","text":"本来基于 kubernetnes : v1.25.4 之前那篇文章对 kubelet 的工作原理做了简单的介绍，接下来，我们对 pod 创建的流程中，kubelet 创建 pod 进行一个详细的分析。 SyncLoop我们来看下这个主循环 SyncLoop： /pkg/kubelet/kubelet.go#2009 func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; klog.InfoS(&quot;Starting kubelet main sync loop&quot;) // The syncTicker wakes up kubelet to checks if there are any pod workers // that need to be sync&#x27;d. A one-second period is sufficient because the // sync interval is defaulted to 10s. syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base // Responsible for checking limits in resolv.conf // The limits do not have anything to do with individual pods // Since this is called in syncLoop, we don&#x27;t need to call it anywhere else if kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123; kl.dnsConfigurer.CheckLimitsForResolvConf() &#125; for &#123; if err := kl.runtimeState.runtimeErrors(); err != nil &#123; klog.ErrorS(err, &quot;Skipping pod synchronization&quot;) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; SyncLoop 起了一个死循环，循环里只调用了 syncLoopIteration 方法。而 syncLoopIteration 会对传入的所有 channel 遍历，发现任何一个管道有消息就交给 handler 去处理。 这些 channel 包括： configCh：该 channel 的生产者为 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时监听来自 file，http，apiserver 的 pod 信息的变化，一旦某个来源的 pod 信息发生了更新，就会向这个 channel 生产相关事件。 plegCh：该 channel 的生产者为 pleg 子模块，该模块会周期性地向容器运行时查询当前所有容器的状态，如果状态发生变化，则向这个 channel 生产事件。 syncCh：定时同步最新保存的 pod 状态。 health manager : livenessManager、readinessManager、startupManager 健康检查发现某个 pod 不可用，Kubelet 将根据 Pod 的 restartPolicy 自动执行正确的操作。 houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作。 syncLoopIteration/pkg/kubelet/kubelet.go#2083 func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123; select &#123; case u, open := &lt;-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open &#123; klog.ErrorS(nil, &quot;Update channel is closed, exiting the sync loop&quot;) return false &#125; switch u.Op &#123; case kubetypes.ADD: klog.V(2).InfoS(&quot;SyncLoop ADD&quot;, &quot;source&quot;, u.Source, &quot;pods&quot;, klog.KObjs(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: klog.V(2).InfoS(&quot;SyncLoop UPDATE&quot;, &quot;source&quot;, u.Source, &quot;pods&quot;, klog.KObjs(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: klog.V(2).InfoS(&quot;SyncLoop REMOVE&quot;, &quot;source&quot;, u.Source, &quot;pods&quot;, klog.KObjs(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: klog.V(4).InfoS(&quot;SyncLoop RECONCILE&quot;, &quot;source&quot;, u.Source, &quot;pods&quot;, klog.KObjs(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: klog.V(2).InfoS(&quot;SyncLoop DELETE&quot;, &quot;source&quot;, u.Source, &quot;pods&quot;, klog.KObjs(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? klog.ErrorS(nil, &quot;Kubelet does not support snapshot update&quot;) default: klog.ErrorS(nil, &quot;Invalid operation type received&quot;, &quot;operation&quot;, u.Op) &#125; kl.sourcesReady.AddSource(u.Source) case e := &lt;-plegCh: if e.Type == pleg.ContainerStarted &#123; // record the most recent time we observed a container start for this pod. // this lets us selectively invalidate the runtimeCache when processing a delete for this pod // to make sure we don&#x27;t miss handling graceful termination for containers we reported as having started. kl.lastContainerStartedTime.Add(e.ID, time.Now()) &#125; if isSyncPodWorthy(e) &#123; // PLEG event for a pod; sync it. if pod, ok := kl.podManager.GetPodByUID(e.ID); ok &#123; klog.V(2).InfoS(&quot;SyncLoop (PLEG): event for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;event&quot;, e) handler.HandlePodSyncs([]*v1.Pod&#123;pod&#125;) &#125; else &#123; // If the pod no longer exists, ignore the event. klog.V(4).InfoS(&quot;SyncLoop (PLEG): pod does not exist, ignore irrelevant event&quot;, &quot;event&quot;, e) &#125; &#125; if e.Type == pleg.ContainerDied &#123; if containerID, ok := e.Data.(string); ok &#123; kl.cleanUpContainersInPod(e.ID, containerID) &#125; &#125; case &lt;-syncCh: // Sync pods waiting for sync podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 &#123; break &#125; klog.V(4).InfoS(&quot;SyncLoop (SYNC) pods&quot;, &quot;total&quot;, len(podsToSync), &quot;pods&quot;, klog.KObjs(podsToSync)) handler.HandlePodSyncs(podsToSync) case update := &lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure &#123; handleProbeSync(kl, update, handler, &quot;liveness&quot;, &quot;unhealthy&quot;) &#125; case update := &lt;-kl.readinessManager.Updates(): ready := update.Result == proberesults.Success kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready) status := &quot;&quot; if ready &#123; status = &quot;ready&quot; &#125; handleProbeSync(kl, update, handler, &quot;readiness&quot;, status) case update := &lt;-kl.startupManager.Updates(): started := update.Result == proberesults.Success kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started) status := &quot;unhealthy&quot; if started &#123; status = &quot;started&quot; &#125; handleProbeSync(kl, update, handler, &quot;startup&quot;, status) case &lt;-housekeepingCh: if !kl.sourcesReady.AllReady() &#123; // If the sources aren&#x27;t ready or volume manager has not yet synced the states, // skip housekeeping, as we may accidentally delete pods from unready sources. klog.V(4).InfoS(&quot;SyncLoop (housekeeping, skipped): sources aren&#x27;t ready yet&quot;) &#125; else &#123; start := time.Now() klog.V(4).InfoS(&quot;SyncLoop (housekeeping)&quot;) if err := handler.HandlePodCleanups(); err != nil &#123; klog.ErrorS(err, &quot;Failed cleaning pods&quot;) &#125; duration := time.Since(start) if duration &gt; housekeepingWarningDuration &#123; klog.ErrorS(fmt.Errorf(&quot;housekeeping took too long&quot;), &quot;Housekeeping took longer than 15s&quot;, &quot;seconds&quot;, duration.Seconds()) &#125; klog.V(4).InfoS(&quot;SyncLoop (housekeeping) end&quot;) &#125; &#125; return true&#125; 创建pod过程Kubelet 创建 pod 的过程是由 configCh 中的 ADD 事件触发的，那么下面主要看下 Kubelet 接收到 ADD 事件后的主要流程。 Handler当 configCh 中出现了 ADD 事件，loop 会触发 SyncHandler 的 HandlePodAdditions 方法。这个方法的流程可以用下面这张流程图描述： 首先 handler 会将所有的 pod 安装创建时间进行排序，然后逐个进行处理。 然后将 pod 添加到 podManager 中，以方便后续操作；然后判断其是否为 mirror pod，如果是将作为 mirror pod 处理，否则按照正常 pod 处理。 这里解释一下 mirror pod： mirror pod 是 static pod 在 kueblet 在 apiserver 创建的一份副本。由于 static pod 是由 Kubelet 直接管理的，apiserver 并不会感知到 static pod 的存在，其生命周期都由 Kubelet 直接托管。为了可以通过 kubectl 命令查看对应的 pod，并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息，Kubelet 通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod。 接着判断 pod 是否能再该节点上运行，也就是所谓的 Kubelet 中的 pod 准入控制，准入控制主要包括这几方面： 节点是否满足 pod 的亲和性规则 节点是否有足够的资源分配给 pod 节点是否使用 HostNetwork 或者 HostIPC，若使用了，是否在节点的白名单里 /proc 挂载目录满足要求 pod 是否配置且是否配置正确的 AppArmor 当所有的条件都满足后，最后触发 podWorker 同步 pod。 HandlePodAdditions 对应的代码如下： /pkg/kubelet/kubelet.go#2238 func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123; start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods &#123; existingPods := kl.podManager.GetPods() // Always add the pod to the pod manager. Kubelet relies on the pod // manager as the source of truth for the desired state. If a pod does // not exist in the pod manager, it means that it has been deleted in // the apiserver and no action (other than cleanup) is required. kl.podManager.AddPod(pod) if kubetypes.IsMirrorPod(pod) &#123; kl.handleMirrorPod(pod, start) continue &#125; // Only go through the admission process if the pod is not requested // for termination by another part of the kubelet. If the pod is already // using resources (previously admitted), the pod worker is going to be // shutting it down. If the pod hasn&#x27;t started yet, we know that when // the pod worker is invoked it will also avoid setting up the pod, so // we simply avoid doing any work. if !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123; // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutInactivePods(existingPods) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123; kl.rejectPod(pod, reason, message) continue &#125; &#125; mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) &#125;&#125; podWorkers 的工作接下来看看 podWorker 的工作。podWorker 维护了一个 map 叫 podUpdates (map[types.UID]chan podWork)，以 pod uid 为 key，为每个 pod 维护一个 channel；当 pod 有事件过来的时候，首先从这个 map 里获取对应的 channel，然后启动一个 goroutine 监听这个 channel，并执行 managePodLoop；另一方面 podWorker 向这个 channel 中传入需要同步的 pod。 managePodLoop 接收到事件后，会先从 pod cache 中获取该 pod 最新的 status，以确保当前处理的 pod 是最新状态；然后调用 syncPod 方法，将其同步后的结果记录在 workQueue 中，等待下一次定时同步任务处理。 整个过程如下图所示： podWorker 中处理 pod 事件的代码： /pkg/kubelet/pod_workers.go#557 func (p *podWorkers) UpdatePod(options UpdatePodOptions) &#123; ... // allow testing of delays in the pod update channel var outCh &lt;-chan podWork if p.workerChannelFn != nil &#123; outCh = p.workerChannelFn(uid, podUpdates) &#125; else &#123; outCh = podUpdates &#125; // Creating a new pod worker either means this is a new pod, or that the // kubelet just restarted. In either case the kubelet is willing to believe // the status of the pod for the first pod worker sync. See corresponding // comment in syncPod. go func() &#123; defer runtime.HandleCrash() p.managePodLoop(outCh) &#125;() &#125; // dispatch a request to the pod worker if none are running if !status.IsWorking() &#123; status.working = true podUpdates &lt;- work return &#125; ...&#125; managePodLoop 函数，会根据 update.WorkType 类型，去分别执行 syncTerminatedPodFn、syncTerminatingPodFn、syncPodFn方法去调用 /pkg/kubelet/pod_worker.go#877 func (p *podWorkers) managePodLoop(podUpdates &lt;-chan podWork) &#123; var lastSyncTime time.Time var podStarted bool for update := range podUpdates &#123; pod := update.Options.Pod ... // Take the appropriate action (illegal phases are prevented by UpdatePod) switch &#123; case update.WorkType == TerminatedPodWork: err = p.syncTerminatedPodFn(ctx, pod, status) case update.WorkType == TerminatingPodWork: var gracePeriod *int64 if opt := update.Options.KillPodOptions; opt != nil &#123; gracePeriod = opt.PodTerminationGracePeriodSecondsOverride &#125; podStatusFn := p.acknowledgeTerminating(pod) err = p.syncTerminatingPodFn(ctx, pod, status, update.Options.RunningPod, gracePeriod, podStatusFn) default: isTerminal, err = p.syncPodFn(ctx, update.Options.UpdateType, pod, update.Options.MirrorPod, status) &#125; lastSyncTime = time.Now() return err &#125;() ... // queue a retry if necessary, then put the next event in the channel if any p.completeWork(pod, phaseTransition, err) if start := update.Options.StartTime; !start.IsZero() &#123; metrics.PodWorkerDuration.WithLabelValues(update.Options.UpdateType.String()).Observe(metrics.SinceInSeconds(start)) &#125; klog.V(4).InfoS(&quot;Processing pod event done&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, pod.UID, &quot;updateType&quot;, update.WorkType) &#125;&#125; syncPod上述 podWorker 在 managePodLoop 中调用的 syncPodFn 方法，其实是 Kubelet 对象的 SyncPod 方法，在文件 pkg/kubelet/kubelet.go 中。 这个方法是真正与 container runtime 层交互的。首先，如果正在创建，记录pod worker启动延迟；设置podIP为hostIP；然后判断是否可以在节点上运行，这里就是上面讲到的 Kubelet 的准入控制；再判断 CNI 插件是否 ready，如果不 ready，则只在 pod 使用 host network 的时候创建并更新 pod 的 cgroups；接着再判断是否是静态 pod，如果是就创建相应的 mirror pod；然后创建 pod 需要挂载的目录；最后调用 runtime 的 syncPod。整个流程如下所示： /pkg/kubelet/kubelet.go#1522 func (kl *Kubelet) syncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) &#123; klog.V(4).InfoS(&quot;syncPod enter&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, pod.UID) defer func() &#123; klog.V(4).InfoS(&quot;syncPod exit&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, pod.UID, &quot;isTerminal&quot;, isTerminal) &#125;() // Latency measurements for the main workflow are relative to the // first time the pod was seen by kubelet. var firstSeenTime time.Time if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok &#123; firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get() &#125; // Record pod worker start latency if being created // TODO: make pod workers record their own latencies if updateType == kubetypes.SyncPodCreate &#123; if !firstSeenTime.IsZero() &#123; // This is the first time we are syncing the pod. Record the latency // since kubelet first saw the pod if firstSeenTime is set. metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime)) &#125; else &#123; klog.V(3).InfoS(&quot;First seen time not recorded for pod&quot;, &quot;podUID&quot;, pod.UID, &quot;pod&quot;, klog.KObj(pod)) &#125; &#125; // Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus) // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576) // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and // set pod IP to hostIP directly in runtime.GetPodStatus podStatus.IPs = make([]string, 0, len(apiPodStatus.PodIPs)) for _, ipInfo := range apiPodStatus.PodIPs &#123; podStatus.IPs = append(podStatus.IPs, ipInfo.IP) &#125; if len(podStatus.IPs) == 0 &amp;&amp; len(apiPodStatus.PodIP) &gt; 0 &#123; podStatus.IPs = []string&#123;apiPodStatus.PodIP&#125; &#125; // If the pod is terminal, we don&#x27;t need to continue to setup the pod if apiPodStatus.Phase == v1.PodSucceeded || apiPodStatus.Phase == v1.PodFailed &#123; kl.statusManager.SetPodStatus(pod, apiPodStatus) isTerminal = true return isTerminal, nil &#125; // If the pod should not be running, we request the pod&#x27;s containers be stopped. This is not the same // as termination (we want to stop the pod, but potentially restart it later if soft admission allows // it later). Set the status and phase appropriately runnable := kl.canRunPod(pod) if !runnable.Admit &#123; // Pod is not runnable; and update the Pod and Container statuses to why. if apiPodStatus.Phase != v1.PodFailed &amp;&amp; apiPodStatus.Phase != v1.PodSucceeded &#123; apiPodStatus.Phase = v1.PodPending &#125; apiPodStatus.Reason = runnable.Reason apiPodStatus.Message = runnable.Message // Waiting containers are not creating. const waitingReason = &quot;Blocked&quot; for _, cs := range apiPodStatus.InitContainerStatuses &#123; if cs.State.Waiting != nil &#123; cs.State.Waiting.Reason = waitingReason &#125; &#125; for _, cs := range apiPodStatus.ContainerStatuses &#123; if cs.State.Waiting != nil &#123; cs.State.Waiting.Reason = waitingReason &#125; &#125; &#125; // Record the time it takes for the pod to become running // since kubelet first saw the pod if firstSeenTime is set. existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok || existingStatus.Phase == v1.PodPending &amp;&amp; apiPodStatus.Phase == v1.PodRunning &amp;&amp; !firstSeenTime.IsZero() &#123; metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime)) &#125; kl.statusManager.SetPodStatus(pod, apiPodStatus) // Pods that are not runnable must be stopped - return a typed error to the pod worker if !runnable.Admit &#123; klog.V(2).InfoS(&quot;Pod is not runnable and must have running containers stopped&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, pod.UID, &quot;message&quot;, runnable.Message) var syncErr error p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err := kl.killPod(pod, p, nil); err != nil &#123; kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, &quot;error killing pod: %v&quot;, err) syncErr = fmt.Errorf(&quot;error killing pod: %v&quot;, err) utilruntime.HandleError(syncErr) &#125; else &#123; // There was no error killing the pod, but the pod cannot be run. // Return an error to signal that the sync loop should back off. syncErr = fmt.Errorf(&quot;pod cannot be run: %s&quot;, runnable.Message) &#125; return false, syncErr &#125; // If the network plugin is not ready, only start the pod if it uses the host network if err := kl.runtimeState.networkErrors(); err != nil &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123; kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, &quot;%s: %v&quot;, NetworkNotReadyErrorMsg, err) return false, fmt.Errorf(&quot;%s: %v&quot;, NetworkNotReadyErrorMsg, err) &#125; // ensure the kubelet knows about referenced secrets or configmaps used by the pod if !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123; if kl.secretManager != nil &#123; kl.secretManager.RegisterPod(pod) &#125; if kl.configMapManager != nil &#123; kl.configMapManager.RegisterPod(pod) &#125; &#125; // Create Cgroups for the pod and apply resource parameters // to them if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() // If pod has already been terminated then we need not create // or update the pod&#x27;s cgroup // TODO: once context cancellation is added this check can be removed if !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123; // When the kubelet is restarted with the cgroups-per-qos // flag enabled, all the pod&#x27;s running containers // should be killed intermittently and brought back up // under the qos cgroup hierarchy. // Check if this is the pod&#x27;s first sync firstSync := true for _, containerStatus := range apiPodStatus.ContainerStatuses &#123; if containerStatus.State.Running != nil &#123; firstSync = false break &#125; &#125; // Don&#x27;t kill containers in pod if pod&#x27;s cgroups already // exists or the pod is running for the first time podKilled := false if !pcm.Exists(pod) &amp;&amp; !firstSync &#123; p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus) if err := kl.killPod(pod, p, nil); err == nil &#123; podKilled = true &#125; else &#123; klog.ErrorS(err, &quot;KillPod failed&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podStatus&quot;, podStatus) &#125; &#125; // Create and Update pod&#x27;s Cgroups // Don&#x27;t create cgroups for run once pod if it was killed above // The current policy is not to restart the run once pods when // the kubelet is restarted with the new flag as run once pods are // expected to run only once and if the kubelet is restarted then // they are not expected to run again. // We don&#x27;t create and apply updates to cgroup if its a run once pod and was killed above if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123; if !pcm.Exists(pod) &#123; if err := kl.containerManager.UpdateQOSCgroups(); err != nil &#123; klog.V(2).InfoS(&quot;Failed to update QoS cgroups while syncing pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err) &#125; if err := pcm.EnsureExists(pod); err != nil &#123; kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, &quot;unable to ensure pod container exists: %v&quot;, err) return false, fmt.Errorf(&quot;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&quot;, pod.UID, err) &#125; &#125; &#125; &#125; // Create Mirror Pod for Static Pod if it doesn&#x27;t already exist if kubetypes.IsStaticPod(pod) &#123; deleted := false if mirrorPod != nil &#123; if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) &#123; // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. klog.InfoS(&quot;Trying to delete pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, mirrorPod.ObjectMeta.UID) podFullName := kubecontainer.GetPodFullName(pod) var err error deleted, err = kl.podManager.DeleteMirrorPod(podFullName, &amp;mirrorPod.ObjectMeta.UID) if deleted &#123; klog.InfoS(&quot;Deleted mirror pod because it is outdated&quot;, &quot;pod&quot;, klog.KObj(mirrorPod)) &#125; else if err != nil &#123; klog.ErrorS(err, &quot;Failed deleting mirror pod&quot;, &quot;pod&quot;, klog.KObj(mirrorPod)) &#125; &#125; &#125; if mirrorPod == nil || deleted &#123; node, err := kl.GetNode() if err != nil || node.DeletionTimestamp != nil &#123; klog.V(4).InfoS(&quot;No need to create a mirror pod, since node has been removed from the cluster&quot;, &quot;node&quot;, klog.KRef(&quot;&quot;, string(kl.nodeName))) &#125; else &#123; klog.V(4).InfoS(&quot;Creating a mirror pod for static pod&quot;, &quot;pod&quot;, klog.KObj(pod)) if err := kl.podManager.CreateMirrorPod(pod); err != nil &#123; klog.ErrorS(err, &quot;Failed creating a mirror pod for&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; &#125; &#125; &#125; // Make data directories for the pod if err := kl.makePodDataDirs(pod); err != nil &#123; kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, &quot;error making pod data directories: %v&quot;, err) klog.ErrorS(err, &quot;Unable to make pod data directories for pod&quot;, &quot;pod&quot;, klog.KObj(pod)) return false, err &#125; // Volume manager will not mount volumes for terminating pods // TODO: once context cancellation is added this check can be removed if !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123; // Wait for volumes to attach/mount if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123; kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, &quot;Unable to attach or mount volumes: %v&quot;, err) klog.ErrorS(err, &quot;Unable to attach or mount volumes for pod; skipping pod&quot;, &quot;pod&quot;, klog.KObj(pod)) return false, err &#125; &#125; // Fetch the pull secrets for the pod pullSecrets := kl.getPullSecretsForPod(pod) // Ensure the pod is being probed kl.probeManager.AddPod(pod) // Call the container runtime&#x27;s SyncPod callback result := kl.containerRuntime.SyncPod(pod, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil &#123; // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults &#123; if r.Error != kubecontainer.ErrCrashLoopBackOff &amp;&amp; r.Error != images.ErrImagePullBackOff &#123; // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime, so we get better errors. return false, err &#125; &#125; return false, nil &#125; return false, nil&#125; 创建容器containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。 /pkg/kubelet/kuberuntime/kuberuntime_manager.go#668 func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123; // Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(pod, podStatus) klog.V(3).InfoS(&quot;computePodActions got for pod&quot;, &quot;podActions&quot;, podContainerChanges, &quot;pod&quot;, klog.KObj(pod)) if podContainerChanges.CreateSandbox &#123; ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil &#123; klog.ErrorS(err, &quot;Couldn&#x27;t make a ref to pod&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; if podContainerChanges.SandboxID != &quot;&quot; &#123; m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, &quot;Pod sandbox changed, it will be killed and re-created.&quot;) &#125; else &#123; klog.V(4).InfoS(&quot;SyncPod received new pod, will create a sandbox for it&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; &#125; // Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod &#123; if podContainerChanges.CreateSandbox &#123; klog.V(4).InfoS(&quot;Stopping PodSandbox for pod, will start new one&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; else &#123; klog.V(4).InfoS(&quot;Stopping PodSandbox for pod, because all other containers are dead&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil &#123; klog.ErrorS(killResult.Error(), &quot;killPodWithSyncResult failed&quot;) return &#125; if podContainerChanges.CreateSandbox &#123; m.purgeInitContainers(pod, podStatus) &#125; &#125; else &#123; // Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill &#123; klog.V(3).InfoS(&quot;Killing unwanted container for pod&quot;, &quot;containerName&quot;, containerInfo.name, &quot;containerID&quot;, containerID, &quot;pod&quot;, klog.KObj(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, containerInfo.reason, nil); err != nil &#123; killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) klog.ErrorS(err, &quot;killContainer for pod failed&quot;, &quot;containerName&quot;, containerInfo.name, &quot;containerID&quot;, containerID, &quot;pod&quot;, klog.KObj(pod)) return &#125; &#125; &#125; // Keep terminated init containers fairly aggressively controlled // This is an optimization because container removals are typically handled // by container garbage collector. m.pruneInitContainersBeforeStart(pod, podStatus) // We pass the value of the PRIMARY podIP and list of podIPs down to // generatePodSandboxConfig and generateContainerConfig, which in turn // passes it to various other functions, in order to facilitate functionality // that requires this value (hosts file and downward API) and avoid races determining // the pod IP in cases where a container requires restart but the // podIP isn&#x27;t in the status manager yet. The list of podIPs is used to // generate the hosts file. // // We default to the IPs in the passed-in pod status, and overwrite them if the // sandbox needs to be (re)started. var podIPs []string if podStatus != nil &#123; podIPs = podStatus.IPs &#125; // Step 4: Create a sandbox for the pod if necessary. podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox &#123; var msg string var err error klog.V(4).InfoS(&quot;Creating PodSandbox for pod&quot;, &quot;pod&quot;, klog.KObj(pod)) metrics.StartedPodsTotal.Inc() createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) // ConvertPodSysctlsVariableToDotsSeparator converts sysctl variable // in the Pod.Spec.SecurityContext.Sysctls slice into a dot as a separator. // runc uses the dot as the separator to verify whether the sysctl variable // is correct in a separate namespace, so when using the slash as the sysctl // variable separator, runc returns an error: &quot;sysctl is not in a separate kernel namespace&quot; // and the podSandBox cannot be successfully created. Therefore, before calling runc, // we need to convert the sysctl variable, the dot is used as a separator to separate the kernel namespace. // When runc supports slash as sysctl separator, this function can no longer be used. sysctl.ConvertPodSysctlsVariableToDotsSeparator(pod.Spec.SecurityContext) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil &#123; // createPodSandbox can return an error from CNI, CSI, // or CRI if the Pod has been deleted while the POD is // being created. If the pod has been deleted then it&#x27;s // not a real error. // // SyncPod can still be running when we get here, which // means the PodWorker has not acked the deletion. if m.podStateProvider.IsPodTerminationRequested(pod.UID) &#123; klog.V(4).InfoS(&quot;Pod was deleted and sandbox failed to be created&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;podUID&quot;, pod.UID) return &#125; metrics.StartedPodsErrorsTotal.Inc() createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg) klog.ErrorS(err, &quot;CreatePodSandbox for pod failed&quot;, &quot;pod&quot;, klog.KObj(pod)) ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil &#123; klog.ErrorS(referr, &quot;Couldn&#x27;t make a ref to pod&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, &quot;Failed to create pod sandbox: %v&quot;, err) return &#125; klog.V(4).InfoS(&quot;Created PodSandbox for pod&quot;, &quot;podSandboxID&quot;, podSandboxID, &quot;pod&quot;, klog.KObj(pod)) resp, err := m.runtimeService.PodSandboxStatus(podSandboxID, false) if err != nil &#123; ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil &#123; klog.ErrorS(referr, &quot;Couldn&#x27;t make a ref to pod&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedStatusPodSandBox, &quot;Unable to get pod sandbox status: %v&quot;, err) klog.ErrorS(err, &quot;Failed to get pod sandbox status; Skipping pod&quot;, &quot;pod&quot;, klog.KObj(pod)) result.Fail(err) return &#125; if resp.GetStatus() == nil &#123; result.Fail(errors.New(&quot;pod sandbox status is nil&quot;)) return &#125; // If we ever allow updating a pod from non-host-network to // host-network, we may use a stale IP. if !kubecontainer.IsHostNetworkPod(pod) &#123; // Overwrite the podIPs passed in the pod status, since we just started the pod sandbox. podIPs = m.determinePodSandboxIPs(pod.Namespace, pod.Name, resp.GetStatus()) klog.V(4).InfoS(&quot;Determined the ip for pod after sandbox changed&quot;, &quot;IPs&quot;, podIPs, &quot;pod&quot;, klog.KObj(pod)) &#125; &#125; // the start containers routines depend on pod ip(as in primary pod ip) // instead of trying to figure out if we have 0 &lt; len(podIPs) // everytime, we short circuit it here podIP := &quot;&quot; if len(podIPs) != 0 &#123; podIP = podIPs[0] &#125; // Get podSandboxConfig for containers to start. configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) if err != nil &#123; message := fmt.Sprintf(&quot;GeneratePodSandboxConfig for pod %q failed: %v&quot;, format.Pod(pod), err) klog.ErrorS(err, &quot;GeneratePodSandboxConfig for pod failed&quot;, &quot;pod&quot;, klog.KObj(pod)) configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message) return &#125; // Helper containing boilerplate common to starting all types of containers. // typeName is a description used to describe this type of container in log messages, // currently: &quot;container&quot;, &quot;init container&quot; or &quot;ephemeral container&quot; // metricLabel is the label used to describe this type of container in monitoring metrics. // currently: &quot;container&quot;, &quot;init_container&quot; or &quot;ephemeral_container&quot; start := func(typeName, metricLabel string, spec *startSpec) error &#123; startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff) if isInBackOff &#123; startContainerResult.Fail(err, msg) klog.V(4).InfoS(&quot;Backing Off restarting container in pod&quot;, &quot;containerType&quot;, typeName, &quot;container&quot;, spec.container, &quot;pod&quot;, klog.KObj(pod)) return err &#125; metrics.StartedContainersTotal.WithLabelValues(metricLabel).Inc() if sc.HasWindowsHostProcessRequest(pod, spec.container) &#123; metrics.StartedHostProcessContainersTotal.WithLabelValues(metricLabel).Inc() &#125; klog.V(4).InfoS(&quot;Creating container in pod&quot;, &quot;containerType&quot;, typeName, &quot;container&quot;, spec.container, &quot;pod&quot;, klog.KObj(pod)) // NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs. if msg, err := m.startContainer(podSandboxID, podSandboxConfig, spec, pod, podStatus, pullSecrets, podIP, podIPs); err != nil &#123; // startContainer() returns well-defined error codes that have reasonable cardinality for metrics and are // useful to cluster administrators to distinguish &quot;server errors&quot; from &quot;user errors&quot;. metrics.StartedContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc() if sc.HasWindowsHostProcessRequest(pod, spec.container) &#123; metrics.StartedHostProcessContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc() &#125; startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch &#123; case err == images.ErrImagePullBackOff: klog.V(3).InfoS(&quot;Container start failed in pod&quot;, &quot;containerType&quot;, typeName, &quot;container&quot;, spec.container, &quot;pod&quot;, klog.KObj(pod), &quot;containerMessage&quot;, msg, &quot;err&quot;, err) default: utilruntime.HandleError(fmt.Errorf(&quot;%v %+v start failed in pod %v: %v: %s&quot;, typeName, spec.container, format.Pod(pod), err, msg)) &#125; return err &#125; return nil &#125; // Step 5: start ephemeral containers // These are started &quot;prior&quot; to init containers to allow running ephemeral containers even when there // are errors starting an init container. In practice init containers will start first since ephemeral // containers cannot be specified on pod creation. for _, idx := range podContainerChanges.EphemeralContainersToStart &#123; start(&quot;ephemeral container&quot;, metrics.EphemeralContainer, ephemeralContainerStartSpec(&amp;pod.Spec.EphemeralContainers[idx])) &#125; // Step 6: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil &#123; // Start the next init container. if err := start(&quot;init container&quot;, metrics.InitContainer, containerStartSpec(container)); err != nil &#123; return &#125; // Successfully started the container; clear the entry in the failure klog.V(4).InfoS(&quot;Completed init container for pod&quot;, &quot;containerName&quot;, container.Name, &quot;pod&quot;, klog.KObj(pod)) &#125; // Step 7: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart &#123; start(&quot;container&quot;, metrics.Container, containerStartSpec(&amp;pod.Spec.Containers[idx])) &#125; return&#125;","categories":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"}],"tags":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"},{"name":"cri","slug":"cri","permalink":"http://lizhewei91.github.io/tags/cri/"}]},{"title":"kubelet volume manager源码分析","slug":"kubelet-volume-manager","date":"2023-03-07T08:39:15.000Z","updated":"2023-03-07T11:41:50.759Z","comments":true,"path":"2023/03/07/15/","link":"","permalink":"http://lizhewei91.github.io/2023/03/07/15/","excerpt":"","text":"","categories":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"}],"tags":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"},{"name":"volume-manager","slug":"volume-manager","permalink":"http://lizhewei91.github.io/tags/volume-manager/"}]},{"title":"如何构建多CPU架构容器镜像","slug":"docker-images-build","date":"2023-02-27T04:47:09.000Z","updated":"2023-03-02T02:54:06.259Z","comments":true,"path":"2023/02/27/09/","link":"","permalink":"http://lizhewei91.github.io/2023/02/27/09/","excerpt":"","text":"构建多架构镜像的方法有两种： manifest docker buildx buildx首先 docker 版本要在 19.03 以上（含），启动 docker buildx，export DOCKER_CLI_EXPERIMENTAL=enabled build-demo示例链接：https://github.com/lizhewei91/buildx-demo [root@build-cloud-product-clone-4 buildx-demo]# tree.├── bin│ └── buildx-demo├── Dockfile.buildx├── go.mod├── main.go└── Makefile Dockerfile.buildx 示例ARG BASE_IMAGEARG BASE_IMAGE_VERSIONFROM --platform=$&#123;TARGETPLATFORM&#125; $&#123;BASE_IMAGE&#125;:$&#123;BASE_IMAGE_VERSION&#125; AS builderWORKDIR /go/src/buildx-demoCOPY . .FROM --platform=$&#123;TARGETPLATFORM&#125; alpine:3.17.2COPY --from=builder /go/src/buildx-demo/bin/buildx-demo /usr/bin/buildx-demoCMD [&quot;/usr/bin/buildx-demo&quot;] Makefile示例TARGET_PLATFORMS ?= linux/amd64,linux/arm64BASE_IMAGE ?= golangBASE_IMAGE_VERSION ?= alpine3.17IMAGE_REPO ?= hub.easystack.cn/multi/buildx-demoIMAGE_VERSION ?= v0.0.1DOCKER_HUB_REPO ?= hub.easystack.cnDOCKER_HUB_USERNAME ?= xxxDOCKER_HUB_PASSWORD ?= xxx.PHONY: allall: docker-hub-login build imagesdocker-hub-login: docker logout docker login $&#123;DOCKER_HUB_REPO&#125; -u $&#123;DOCKER_HUB_USERNAME&#125; -p $&#123;DOCKER_HUB_PASSWORD&#125;build: go build -o /go/src/buildx-demo/bin/buildx-demo main.goimages: docker buildx build \\ --build-arg BASE_IMAGE=$(BASE_IMAGE) \\ --build-arg BASE_IMAGE_VERSION=$(BASE_IMAGE_VERSION) \\ --platform $(TARGET_PLATFORMS) \\ -t $(IMAGE_REPO):$(IMAGE_VERSION) \\ -f ./Dockfile.buildx --push . 验证镜像// 拉去镜像到本地$ docker pull hub.easystack.cn/multi/buildx-demo:v0.0.1// 查看镜像信息$ docker manifest inspect hub.easystack.cn/multi/buildx-demo:v0.0.1&#123; &quot;schemaVersion&quot;: 2, &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.list.v2+json&quot;, &quot;manifests&quot;: [ &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 701, &quot;digest&quot;: &quot;sha256:7d95c40e65abaa83c9d1a2e462026070f43f7ceaacba57f3c6a70fb840bcd196&quot;, &quot;platform&quot;: &#123; &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;linux&quot; &#125; &#125;, &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 701, &quot;digest&quot;: &quot;sha256:ff6b7d6df4241220f9c1f665864e7f009620a8c39edecf3b8463a62d01c6f228&quot;, &quot;platform&quot;: &#123; &quot;architecture&quot;: &quot;arm64&quot;, &quot;os&quot;: &quot;linux&quot; &#125; &#125; ]&#125; manifestmanifest是什么，干什么用？manifest是一个文件，这个文件包含了有关于镜像信息，如层、大小和摘要。docker manifest命令还向用户提供附加信息，比如构建镜像的操作系统和体系结构。而manifest list是一个镜像清单列表，用于存放多个不同os/arch的镜像信息。我们可以创建一个manifest list来指向两个镜像(一个linux 64位和一个指向arm64位的镜像)，然后对用户提供一个唯一的镜像名称。从Docker registry v2.3和Docker 1.10 开始，Docker hub就可以pull multi architecture Docker镜像了。 一个镜像的manifest文件信息如下： [root@localhost ~]# docker manifest inspect java&#123; &quot;schemaVersion&quot;: 2, &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;config&quot;: &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.container.image.v1+json&quot;, &quot;size&quot;: 4733, &quot;digest&quot;: &quot;sha256:d23bdf5b1b1b1afce5f1d0fd33e7ed8afbc084b594b9ccf742a5b27080d8a4a8&quot; &#125;, &quot;layers&quot;: [ #---镜像层的摘要信息 &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;, &quot;size&quot;: 51361210, &quot;digest&quot;: &quot;sha256:5040bd2983909aa8896b9932438c3f1479d25ae837a5f6220242a264d0221f2d&quot; &#125;,................... ]&#125; 一个manifest list的例子如下： &#123; &quot;schemaVersion&quot;: 2, &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.list.v2+json&quot;, &quot;manifests&quot;: [ &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 1357, &quot;digest&quot;: &quot;sha256:9b47044b1e79b965a8e1653e7f9c04b5f63e00b9161bedd5baef69bb8b4c4834&quot;, &quot;platform&quot;: &#123; &quot;architecture&quot;: &quot;amd64&quot;, &quot;os&quot;: &quot;linux&quot; &#125; &#125;, &#123; &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;, &quot;size&quot;: 1357, &quot;digest&quot;: &quot;sha256:8aecae775e1f81d3889929ef15647187b414c833b0798d060bfd778bee668ced&quot;, &quot;platform&quot;: &#123; &quot;architecture&quot;: &quot;arm64&quot;, &quot;os&quot;: &quot;linux&quot;, &quot;variant&quot;: &quot;v8&quot; &#125; &#125; ]&#125; 注意：manifest的功能目前仅仅作用于docker 官方的镜像仓库。 总结：简单的说manifest list就是多个manifest的一个集合，通过列表方式来管理。 manifest list处理流程： 开启docker子命令manifest功能：manifest是做为docker客户端的子命令存在，不过这个子命令目前处在实验性中一般没有开启。我们需要手动开始这个子命令的功能。开启过程如下： 编辑config.jsondocker 的默认配置文件config.json是在$HOME目录下的.docker目录下。编辑config.json文件，若目录和文件不存在手动创建。 $ vim ~/.docker/config.json&#123; &quot;experimental&quot;: &quot;enabled&quot;&#125; 编辑daemon.json编辑daemon.json，若目录和文件不存在手动创建 $ vim /etc/docker/daemon.json&#123; &quot;experimental&quot;: true&#125; 重启docker$ systemctl daemon-reload$ systemctl restart docker$ docker manifest --help #----查看manifest帮助信息 开启docker的实验性功能后docker pull可以拉取指定平台镜像如下： $ docker pull --platform arm64 镜像 --platform：该参数是用于拉取指定平台的镜像，也是实验性功能，在上面步骤中开启后就会出现。通 过该参数可以手动指定需要的CPU平台镜像，而不用自动去识别。 使用manifest创建多CPU架构的镜像：查看一个镜像的manifest文件信息 $ docker manifest inspect nginx 查看一个镜像的manifest文件的详细信息，包括cpu平台架构等信息 $ docker manifest inspect --verbose nginx 准备工作这里准备好了两个不同CPU架构的镜像如下：这里的镜像是自己在docker hub上创建的仓库xxx/public_docker:nginx-arm64xxx/public_docker:nginx-x86必须将上面两个镜像推到docker hub上面 创建一个manifest list列表：创建一个自定义命名的镜像名的manifest list，然后用该列表关联仓库里面的两个不同架构的镜像 $ docker manifest create xxx/public_docker:nginx-v1 xxx/public_docker:nginx-arm64 xxx/public_docker:nginx-x86 docker manifest annotate 注释manifest list我们按照上述方法创建出来的 manifest list 中并没有说明其中的 manifest 是什么操作系统和平台的，docker manifest annotate 命令用于注释创建出来的 manifest list。例如注释某个 manifest 是 linxu系统 arm64 平台的。 $ docker manifest annotate xxx/public_docker:nginx-v1 xxx/public_docker:nginx-x86 –os linux –arch amd64$ docker manifest annotate xxx/public_docker:nginx-v1 xxx/public_docker:nginx-arm64 –os linux –arch arm64 将创建好的manifest list 推到仓库中：$ docker manifest push xxx/public_docker:nginx-v1 查看仓库中创建好的manifest list：$ docker manifest inspect xxx/public_docker:nginx-v1","categories":[{"name":"buildx","slug":"buildx","permalink":"http://lizhewei91.github.io/categories/buildx/"}],"tags":[{"name":"buildx","slug":"buildx","permalink":"http://lizhewei91.github.io/tags/buildx/"},{"name":"manifest","slug":"manifest","permalink":"http://lizhewei91.github.io/tags/manifest/"}]},{"title":"在k8s中，实现应用配置文件热更新","slug":"k8s-app-reloader","date":"2023-02-10T08:32:44.000Z","updated":"2023-02-10T10:03:47.949Z","comments":true,"path":"2023/02/10/44/","link":"","permalink":"http://lizhewei91.github.io/2023/02/10/44/","excerpt":"","text":"背景目前，在k8s部署的工作负载使用 ConfigMap 或 Secret时，通过两种方式： 环境变量 Env 方式挂载 文件方式挂载 当更新 ConfigMap 或 Secret 时，挂载到Pod中的数据存在两种情况： Env 方式挂载的环境变量不会同步更新 文件方式挂载的数据会同步更新（存在秒级延时） 大部分场景下，在更新了 ConfigMap 或 Secret 中的信息后，都希望Pod内业务能读取到最新的值。通常都会手动去滚动更新一下Pod，重新读取环境变量或文件内容。当前社区已经有对应的开源工具 Reloader 实现了 ConfigMap/Secret 更新时自动触发Pod的滚动更新。 Reloader介绍Reloader 通过 watch ConfigMap 和 Secret 中的变化，对 Deployment、 DaemonSet 和 StatefulSet 等负载的 Pod 进行滚动升级。 官方文档：Reloader 兼容性Reloader 兼容的 K8s 版本为 &gt;=1.9。 常用配置以下使用Deployment举例： Deployment 中使用的所有 ConfigMap 和 Secret 变动都会触发 Pod 滚动更新 在Deployment 的 metadata.annotations 中添加 reloader.stakater.com/auto: &quot;true&quot; kind: Deploymentmetadata: annotations: reloader.stakater.com/auto: &quot;true&quot;spec: template: Deployment 中的部分 ConfigMap 和 Secret 变动会触发 Pod 滚动更新 在 Deployment 的 metadata.annotations中添加reloader.stakater.com/search: &quot;true&quot; kind: Deploymentmetadata: annotations: reloader.stakater.com/search: &quot;true&quot;spec: template: 同时在需要触发 Pod 更新的 ConfigMap 或 Secret 中的 metadata.annotations中添加 reloader.stakater.com/match: &quot;true&quot; kind: ConfigMapmetadata: annotations: reloader.stakater.com/match: &quot;true&quot;data: key: value Deployment 中指定 ConfigMap 或 Secret 的变动触发 Pod 滚动更新 在Deployment 的 metadata.annotations中添加 configmap.reloader.stakater.com/reload: &quot;foo-configmap,bar-configmap,baz-configmap&quot;，指定这些 ConfigMap 才会触发 Pod 的更新。多个 ConfigMap 使用逗号分隔 kind: Deploymentmetadata: annotations: configmap.reloader.stakater.com/reload: &quot;foo-configmap,bar-configmap,baz-configmap&quot;spec: template: metadata: 在Deployment 的 metadata.annotations 中添加 secret.reloader.stakater.com/reload: &quot;foo-secret,bar-secret,baz-secret&quot;，指定这些 Secret 才会触发 Pod 的更新。多个 Secret 使用逗号分隔 kind: Deploymentmetadata: annotations: secret.reloader.stakater.com/reload: &quot;foo-secret,bar-secret,baz-secret&quot;spec: template: metadata: 其他配置 忽略 ConfigMap 或 Secret 变动（全局） 在 Reloader deployment的 spec.template.spec.container.args 中添加参数： 参数 描述 –resources-to-ignore=configMaps 忽略 configMaps 变动 –resources-to-ignore=secrets 忽略 secrets 变动 --resources-to-ignore参数只支持忽略一种资源，若要同时忽略 configMaps 和 secrets 的变动，则只需要把 Reloader 副本数降为0。 通过使用 --namespace-selector 参数，Reloader可以配置为只监视带有(一个或多个)标签的命名空间，例如： --namespace-selector=reloder:enabled,test:true 只有标记为如下命名空间 YAML 的命名空间才会被监视： kind: NamespaceapiVersion: v1metadata: ... labels: reloder: enabled test: true ... 如果您只想通过标签的键选择命名空间，请使用 &quot;*&quot; 作为值。例如，对于 --namespace-selector=select-this:* 所有标签键为&quot;select-this&quot;的命名空间将被选中，而不管标签的值是多少。 实战验证测试准备 创建一个测试 namespace $ kubectl create ns ns1 安装 reloader $ kubectl apply -f https://raw.githubusercontent.com/stakater/Reloader/master/deployments/kubernetes/reloader.yaml 创建configmap apiVersion: v1kind: ConfigMapmetadata: name: test-config-file namespace: ns1 annotations:# reloader.stakater.com/match: &quot;true&quot;data: info.yaml: | user=lzw age=30---apiVersion: v1kind: ConfigMapmetadata: name: test-config-env namespace: ns1 annotations:# reloader.stakater.com/match: &quot;true&quot;data: COUNTRY: china CITY: beijing 创建 deployment apiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: ns1 annotations:# configmap.reloader.stakater.com/reload: &quot;test-config-file&quot;# reloader.stakater.com/search: &quot;true&quot; reloader.stakater.com/auto: &quot;true&quot;spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx env: # Define the environment variable - name: COUNTRY valueFrom: configMapKeyRef: name: test-config-env key: COUNTRY - name: CITY valueFrom: configMapKeyRef: name: test-config-env key: CITY volumeMounts: - name: foo mountPath: &quot;/etc/foo&quot; readOnly: true volumes: - name: foo configMap: name: test-config-file Reloader功能验证 更新 configmap（以文件方式挂载） 使用命令 kubectl -nns1 edit cm test-config-file 编辑configmap，设置 user 的值为 ted。查看pod已经滚动更新。 ➜ kubectl edit cm -n ns1 test-config-fileconfigmap/test-config-file edited➜ kubectl get pods -n ns1NAME READY STATUS RESTARTS AGEnginx-5dff48f5dd-m528h 0/1 ContainerCreating 0 6snginx-6f455f8cd5-9h7pp 1/1 Running 0 34m 查看新启动 pod 中 configmap 所挂载的文件内容，发现 user 的值已经变为 lzw-test。 ➜ kubectl exec -it nginx-5dff48f5dd-m528h -nns1 -- cat /etc/foo/info.yamluser=lzw-testage=30 更新 configmap（以环境变量方式注入） 使用命令 kubectl -nns1edit cm test-config-env 编辑 configmap，设置 CITY 的值为 shenzhen。查看 pod 已经滚动更新。 ➜ kubectl edit cm -n ns1 test-config-envconfigmap/test-config-env edited➜ kubectl get pods -n ns1NAME READY STATUS RESTARTS AGEnginx-5b4cb86669-9cv6k 0/1 ContainerCreating 0 6snginx-5dff48f5dd-m528h 1/1 Running 0 9m35s 查看新启动 pod 中 configmap 所注入的环境变量，发现 CITY 的值已经变为 shenzhen。 ➜ kubectl exec -it nginx-5b4cb86669-9cv6k -nns1 -- env|grep CITYCITY=shenzhen 注意事项 Reloader 自动触发滚动更新，可能会导致业务中断。使用该功能时需要评估 pod 滚动更新对业务带来的影响。 reloader.stakater.com/auto的优先级高于 reloader.stakater.com/search。","categories":[{"name":"reloader","slug":"reloader","permalink":"http://lizhewei91.github.io/categories/reloader/"}],"tags":[{"name":"reloader","slug":"reloader","permalink":"http://lizhewei91.github.io/tags/reloader/"},{"name":"configMap","slug":"configMap","permalink":"http://lizhewei91.github.io/tags/configMap/"},{"name":"secret","slug":"secret","permalink":"http://lizhewei91.github.io/tags/secret/"}]},{"title":"用插件扩展 kubectl","slug":"kubectl-plugins","date":"2023-02-10T06:48:25.000Z","updated":"2023-02-10T07:13:34.817Z","comments":true,"path":"2023/02/10/25/","link":"","permalink":"http://lizhewei91.github.io/2023/02/10/25/","excerpt":"","text":"krew准备开始 你需要安装一个可用的 kubectl 可执行文件。 安装 krew官方文档：https://krew.sigs.k8s.io/docs/user-guide/ 安装 krew 插件管理器。Krew 是一个由 Kubernetes SIG CLI 社区维护的插件管理器。 Make sure that git is installed. Run this command to download and install krew: ( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; OS=&quot;$(uname | tr &#x27;[:upper:]&#x27; &#x27;[:lower:]&#x27;)&quot; &amp;&amp; ARCH=&quot;$(uname -m | sed -e &#x27;s/x86_64/amd64/&#x27; -e &#x27;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/&#x27; -e &#x27;s/aarch64$/arm64/&#x27;)&quot; &amp;&amp; KREW=&quot;krew-$&#123;OS&#125;_$&#123;ARCH&#125;&quot; &amp;&amp; curl -fsSLO &quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; tar zxvf &quot;$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; ./&quot;$&#123;KREW&#125;&quot; install krew) Add the $HOME/.krew/bin directory to your PATH environment variable. To do this, update your .bashrc or .zshrc file and append the following line: export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot; and restart your shell. Run kubectl krew to check the installation. 编写 kubectl 插件你可以用任何编程语言或脚本编写插件，允许你编写命令行命令。 不需要安装插件或预加载，插件可执行程序从 kubectl 二进制文件接收继承的环境， 插件根据其名称确定它希望实现的命令路径。 例如，名为 kubectl-foo 的插件提供了命令 kubectl foo。 必须将插件的可执行文件安装在 PATH 中的某个位置。 示例插件#!/bin/bash# 可选的参数处理if [[ &quot;$1&quot; == &quot;version&quot; ]]then echo &quot;1.0.0&quot; exit 0fi# 可选的参数处理if [[ &quot;$1&quot; == &quot;config&quot; ]]then echo $KUBECONFIG exit 0fiecho &quot;I am a plugin named kubectl-foo&quot; 使用插件要使用某插件，先要使其可执行： $ sudo chmod +x ./kubectl-foo 并将它放在你的 PATH 中的任何地方： $ sudo mv ./kubectl-foo /usr/local/bin 你现在可以调用你的插件作为 kubectl 命令： $ kubectl fooI am a plugin named kubectl-foo 所有参数和标记按原样传递给可执行文件： $ kubectl foo version1.0.0 所有环境变量也按原样传递给可执行文件： $ KUBECONFIG=/etc/kube/config kubectl foo config/etc/kube/config 此外，传递给插件的第一个参数总是调用它的位置的绝对路径（在上面的例子中，$0 将等于 /usr/local/bin/kubectl-foo）。 查看插件列表$ kubectl plugin listThe following compatible plugins are available:/root/.krew/bin/kubectl-krew/usr/local/bin/kubectl-foo // kubectl-foo 为新添加的插件 分发 kubectl 插件参考：分发kubectl插件 krew 相关命令 升级 $ kubectl krew update 查看可用插件列表 $ kubectl krew searchNAME DESCRIPTION INSTALLEDaccess-matrix Show an RBAC access matrix for server resources noadvise-psp Suggests PodSecurityPolicies for cluster. noauth-proxy Authentication proxy to a pod or service no... 关键字搜索 $ kubectl krew search podNAME DESCRIPTION INSTALLEDevict-pod Evicts the given pod nopod-dive Shows a pod&#x27;s workload tree and info inside a node nopod-logs Display a list of pods to get logs from nopod-shell Display a list of pods to execute a shell in no 查看插件详情 $ kubectl krew info treeNAME: treeVERSION: v0.4.0DESCRIPTION: This plugin shows sub-resources of a specified Kubernetes API object in a tree view in the command-line. The parent-child relationship is discovered using ownerReferences on the child object.","categories":[{"name":"krew","slug":"krew","permalink":"http://lizhewei91.github.io/categories/krew/"}],"tags":[{"name":"kubectl-plugins","slug":"kubectl-plugins","permalink":"http://lizhewei91.github.io/tags/kubectl-plugins/"},{"name":"krew","slug":"krew","permalink":"http://lizhewei91.github.io/tags/krew/"}]},{"title":"kubelet-device-manager 源码分析","slug":"kubelet-device-manager","date":"2023-01-16T06:24:19.000Z","updated":"2023-01-16T09:57:48.270Z","comments":true,"path":"2023/01/16/19/","link":"","permalink":"http://lizhewei91.github.io/2023/01/16/19/","excerpt":"","text":"创建 DeviceManagerDevice Manager 和 cgroup Manager、QoS Container Manager 等一样，都属于 kubelet 管理的众多 Manager 之一。Device Manager在 kubelet 启动时的 NewContainerManager 中创建。 kubernetes/pkg/kubelet/cm/container_manager_linux.go#198 func NewContainerManager(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, nodeConfig NodeConfig, failSwapOn bool, devicePluginEnabled bool, recorder record.EventRecorder) (ContainerManager, error) &#123; ... cm := &amp;containerManagerImpl&#123; cadvisorInterface: cadvisorInterface, mountUtil: mountUtil, NodeConfig: nodeConfig, subsystems: subsystems, cgroupManager: cgroupManager, capacity: capacity, internalCapacity: internalCapacity, cgroupRoot: cgroupRoot, recorder: recorder, qosContainerManager: qosContainerManager, &#125; ... klog.InfoS(&quot;Creating device plugin manager&quot;, &quot;devicePluginEnabled&quot;, devicePluginEnabled) if devicePluginEnabled &#123; cm.deviceManager, err = devicemanager.NewManagerImpl(machineInfo.Topology, cm.topologyManager) cm.topologyManager.AddHintProvider(cm.deviceManager) &#125; else &#123; cm.deviceManager, err = devicemanager.NewManagerStub() &#125; if err != nil &#123; return nil, err &#125; ... return cm, nil&#125; ManagerImpl结构体我们有必要先了解 Device Manager 的结构体： kubernetes/pkg/kubelet/cm/devicemanager/manager.go#57 type ManagerImpl struct &#123; checkpointdir string endpoints map[string]endpointInfo // Key is ResourceName mutex sync.Mutex server plugin.Server // activePods is a method for listing active pods on the node // so the amount of pluginResources requested by existing pods // could be counted when updating allocated devices activePods ActivePodsFunc // sourcesReady provides the readiness of kubelet configuration sources such as apiserver update readiness. // We use it to determine when we can purge inactive pods from checkpointed state. sourcesReady config.SourcesReady // allDevices holds all the devices currently registered to the device manager allDevices ResourceDeviceInstances // healthyDevices contains all of the registered healthy resourceNames and their exported device IDs. healthyDevices map[string]sets.String // unhealthyDevices contains all of the unhealthy devices and their exported device IDs. unhealthyDevices map[string]sets.String // allocatedDevices contains allocated deviceIds, keyed by resourceName. allocatedDevices map[string]sets.String // podDevices contains pod to allocated device mapping. podDevices *podDevices checkpointManager checkpointmanager.CheckpointManager // List of NUMA Nodes available on the underlying machine numaNodes []int // Store of Topology Affinties that the Device Manager can query. topologyAffinityStore topologymanager.Store // devicesToReuse contains devices that can be reused as they have been allocated to // init containers. devicesToReuse PodReusableDevices // pendingAdmissionPod contain the pod during the admission phase pendingAdmissionPod *v1.Pod&#125; 下面是核心字段 的说明： checkpointdir：kubelet对外暴露的socket文件，/var/lib/kubelet/device-plugins/kubelet.sock endpoints：map对象，key 为 Resource Name，value 为 endpoint 接口( 包括 getPreferredAllocation、allocate、preStartContainer、setStopTime、isStopped、stopGracePeriodExpired )，每个 endpoint 接口对应一个已注册的 device plugin，负责与 device plugin 的 gRPC 通信及缓存 device plugin 反馈的 device states。 server：暴漏一个 gRPC 服务。 activePods：用来获取该节点上所有 active pods，即 non-Terminated 状态的 Pods。在 kubelet 的initializeRuntimeDependentModules 时会注册 activePods Func 为如下函数 func (kl *Kubelet) GetActivePods() []*v1.Pod &#123; allPods := kl.podManager.GetPods() activePods := kl.filterOutInactivePods(allPods) return activePods&#125; allDevices：保存当前注册到设备管理器的所有设备 healthyDevices: map对象，key为 Resource Name，value为对应的健康的 device IDs。 **unhealthyDevices: **map对象，key为 Resource Name，value为对应的不健康的 device IDs。 allocatedDevices: map对象，key为Resource Name，value为已经分配出去的device IDs。 podDevices：记录每一个pod中每个容器的设备分配情况","categories":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"}],"tags":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"},{"name":"device-manager","slug":"device-manager","permalink":"http://lizhewei91.github.io/tags/device-manager/"}]},{"title":"kubelet 工作原理分析","slug":"kubelet","date":"2023-01-11T11:32:14.000Z","updated":"2023-03-08T02:10:28.569Z","comments":true,"path":"2023/01/11/14/","link":"","permalink":"http://lizhewei91.github.io/2023/01/11/14/","excerpt":"","text":"概述在kubernetes集群中，每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。 kubelet 主要功能kubelet 默认监听四个端口，分别为 10250 、10255、10248、4194。 LISTEN 0 128 *:10250 *:* users ((&quot;kubelet&quot;,pid=48500,fd=28))LISTEN 0 128 *:10255 *:* users:((&quot;kubelet&quot;,pid=48500,fd=26))LISTEN 0 128 *:4194 *:* users:((&quot;kubelet&quot;,pid=48500,fd=13))LISTEN 0 128 127.0.0.1:10248 *:* users:((&quot;kubelet&quot;,pid=48500,fd=23)) 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 --healthz-port 和 --healthz-bind-address 来指定监听的地址和端口。 $ curl http://127.0.0.1:10248/healthz ok 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 --cadvisor-port 可以指定启动的端口。 $ curl http://127.0.0.1:4194/metrics 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。 // 获取 pod 的接口，与 apiserver 的 // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName= 接口类似$ curl http://127.0.0.1:10255/pods// 节点信息接口,提供磁盘、网络、CPU、内存等信息$ curl http://127.0.0.1:10255/spec/ pod管理Kubelet 以 PodSpec 的方式工作。PodSpec 是描述一个 Pod 的 YAML 或 JSON 对象。 kubelet 采用一组通过各种机制提供的 PodSpecs（主要通过 apiserver），并确保这些 PodSpecs 中描述的 Pod 正常健康运行。 官方提供了3种方式来获取容器信息： apiserver：通过 API Server 监听 etcd 目录获取数据； File：启动参数 –config 指定的配置目录下的文件； 通过 url 从网络上某个地址来获取信息 拿apiserver来说，如果Kubelet 监听到etcd中有新的绑定到本节点的 Pod，则按照 Pod 清单的要求创建该 Pod；如果发现本地的 Pod 被修改，则 Kubelet 会做出相应的修改。 容器健康检查容器健康检查这个我们在前面已经聊过，主要是通过LivenessProbe 与ReadinessProbe来判断容器是否健康。 LivenessProbe ：用于判断容器是否健康，告诉 Kubelet 一个容器什么时候处于不健康的状态。如果 LivenessProbe 探针探测到容器不健康，则 Kubelet 将删除该容器，并根据容器的重启策略做相应的处理。如果一个容器不包含 LivenessProbe 探针，那么 Kubelet 认为该容器的 LivenessProbe 探针返回的值永远是 “Success”； ReadinessProbe：用于判断容器是否启动完成且准备接收请求。如果 ReadinessProbe 探针探测到失败，则 Pod 的状态将被修改。Endpoint Controller 将从 Service 的 Endpoint 中删除包含该容器所在 Pod 的 IP 地址的 Endpoint 条目。 容器监控Kubelet 通过 cAdvisor 获取其所在节点及容器的数据。cAdvisor 是一个开源的分析容器资源使用率和性能特性的代理工具，集成到 Kubelet中，当Kubelet启动时会同时启动cAdvisor，且一个cAdvisor只监控一个Node节点的信息。cAdvisor 自动查找所有在其所在节点上的容器，自动采集 CPU、内存、文件系统和网络使用的统计信息。cAdvisor 通过它所在节点机的 Root 容器，采集并分析该节点机的全面使用情况。 kubelet组件中的模块 kubelet 工作原理其实，kubelet也是按照“控制器”的模式来工作的，它的实际工作原理，可以用如下的一张图表示： 可以看到，kubelet的工作核心，就是一个控制循环，即: SyncLoop ( 图中的大圆圈 )。而驱动这个控制循环的事件，包括四种： Pod 更新事件 Pod 生命周期变化 kubelet 本身设置的执行周期 定时的清理事件","categories":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"}],"tags":[{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"}]},{"title":"NVIDIA/kubevirt-gpu-device-plugin源码分析","slug":"kubevirt-gpu-device-plugin","date":"2023-01-10T08:38:32.000Z","updated":"2023-01-11T09:43:32.630Z","comments":true,"path":"2023/01/10/32/","link":"","permalink":"http://lizhewei91.github.io/2023/01/10/32/","excerpt":"","text":"NVIDIA K8s Device Plugin为Kubevirt虚拟机分配gpu和vgpu，该篇文章基于NVIDIA/kubevirt-gpu-device-plugin:v1.2.1，https://github.com/NVIDIA/kubevirt-gpu-device-plugin/tree/v1.2.1 kubevirt-gpu-device-plugin启动还是一样的套路，一切从main.go开始 Kubevirt-gpu-device-plugin/cmd/main.go#33 func main() &#123; device_plugin.InitiateDevicePlugin()&#125; main函数调用 InitiateDevicePlugin 函数，直接看 InitiateDevicePlugin Kubevirt-gpu-device-plugin/pkg/device_plugin/device_plugin.go#73 func InitiateDevicePlugin() &#123; //Identifies GPUs and represents it in appropriate structures createIommuDeviceMap() //Identifies vGPUs and represents it in appropriate structures createVgpuIDMap() //Creates and starts device plugin createDevicePlugins()&#125; InitiateDevicePlugin 函数主要做三件事： 发现所有加载了 VFIO-PCI 驱动程序的 Nvidia gpu，并创建相应的映射 发现节点上配置的所有 Nvidia vgpu，并创建相应的映射 创建并启动 device-plugin CreateIommuDeviceMapKubevirt-gpu-device-plugin/pkg/device-plugin/device-plugin.go#155 // Discovers all Nvidia GPUs which are loaded with VFIO-PCI driver and creates corresponding mapsfunc createIommuDeviceMap() &#123; iommuMap = make(map[string][]NvidiaGpuDevice) deviceMap = make(map[string][]string) //Walk directory to discover pci devices filepath.Walk(basePath, func(path string, info os.FileInfo, err error) error &#123; if err != nil &#123; log.Printf(&quot;Error accessing file path %q: %v\\n&quot;, path, err) return err &#125; if info.IsDir() &#123; log.Println(&quot;Not a device, continuing&quot;) return nil &#125; //Retrieve vendor for the device vendorID, err := readIDFromFile(basePath, info.Name(), &quot;vendor&quot;) if err != nil &#123; log.Println(&quot;Could not get vendor ID for device &quot;, info.Name()) return nil &#125; //Nvidia vendor id is &quot;10de&quot;. Proceed if vendor id is 10de if vendorID == &quot;10de&quot; &#123; log.Println(&quot;Nvidia device &quot;, info.Name()) //Retrieve iommu group for the device driver, err := readLink(basePath, info.Name(), &quot;driver&quot;) if err != nil &#123; log.Println(&quot;Could not get driver for device &quot;, info.Name()) return nil &#125; if driver == &quot;vfio-pci&quot; &#123; iommuGroup, err := readLink(basePath, info.Name(), &quot;iommu_group&quot;) if err != nil &#123; log.Println(&quot;Could not get IOMMU Group for device &quot;, info.Name()) return nil &#125; log.Println(&quot;Iommu Group &quot; + iommuGroup) _, exists := iommuMap[iommuGroup] if !exists &#123; deviceID, err := readIDFromFile(basePath, info.Name(), &quot;device&quot;) if err != nil &#123; log.Println(&quot;Could get deviceID for PCI address &quot;, info.Name()) return nil &#125; log.Printf(&quot;Device Id %s&quot;, deviceID) deviceMap[deviceID] = append(deviceMap[deviceID], iommuGroup) &#125; iommuMap[iommuGroup] = append(iommuMap[iommuGroup], NvidiaGpuDevice&#123;info.Name()&#125;) &#125; &#125; return nil &#125;)&#125; createIommuDeviceMap的主要流程如下图： CreateVgpuIDMapKubevirt-gpu-device-plugin/pkg/device-plugin/device-plugin.go#208 // Discovers all Nvidia vGPUs configured on a node and creates corresponding mapsfunc createVgpuIDMap() &#123; vGpuMap = make(map[string][]NvidiaGpuDevice) gpuVgpuMap = make(map[string][]string) //Walk directory to discover vGPU devices filepath.Walk(vGpuBasePath, func(path string, info os.FileInfo, err error) error &#123; if err != nil &#123; log.Printf(&quot;Error accessing file path %q: %v\\n&quot;, path, err) return err &#125; if info.IsDir() &#123; log.Println(&quot;Not a device, continuing&quot;) return nil &#125; //Read vGPU type name vGpuID, err := readVgpuIDFromFile(vGpuBasePath, info.Name(), &quot;mdev_type/name&quot;) if err != nil &#123; log.Println(&quot;Could not get vGPU type identifier for device &quot;, info.Name()) return nil &#125; //Retrieve the gpu ID for this vGPU gpuID, err := readGpuIDForVgpu(vGpuBasePath, info.Name()) if err != nil &#123; log.Println(&quot;Could not get vGPU type identifier for device &quot;, info.Name()) return nil &#125; log.Printf(&quot;Gpu id is %s&quot;, gpuID) log.Printf(&quot;Vgpu id is %s&quot;, vGpuID) gpuVgpuMap[gpuID] = append(gpuVgpuMap[gpuID], info.Name()) vGpuMap[vGpuID] = append(vGpuMap[vGpuID], NvidiaGpuDevice&#123;info.Name()&#125;) return nil &#125;)&#125; 通过filePath.Walk遍历“/sys/bus/mdev/devices”目录下的所有文件，得到所有 vgpu 相应设备文件 读取readVgpuIDFromFile(“/sys/bus/mdev/devices”,info.Name(),”mdev_type/name”)，获得vgpu的vGpuID 读取readGpuIDFromVgpu(“/sys/bus/mdev/devices”,info.Name())，获取vgpu对应的gpuID 最后，通过 gpuVgpuMap=map[ ] []string{ ,…} 和 vGpuMap=map[ ] []NvidiaGpuDevice{ {addr:},{addr:}} 存储映射关系。 CreateDevicePluginsKubevirt-gpu-device-plugin/pkg/device-plugin/device-plugin.go#82 // Starts gpu pass through and vGPU device pluginfunc createDevicePlugins() &#123; var devicePlugins []*GenericDevicePlugin var vGpuDevicePlugins []*GenericVGpuDevicePlugin var devs []*pluginapi.Device log.Printf(&quot;Iommu Map %s&quot;, iommuMap) log.Printf(&quot;Device Map %s&quot;, deviceMap) log.Println(&quot;vGPU Map &quot;, vGpuMap) log.Println(&quot;GPU vGPU Map &quot;, gpuVgpuMap) //Iterate over deivceMap to create device plugin for each type of GPU on the host for k, v := range deviceMap &#123; devs = nil for _, dev := range v &#123; devs = append(devs, &amp;pluginapi.Device&#123; ID: dev, Health: pluginapi.Healthy, &#125;) &#125; deviceName := getDeviceName(k) if deviceName == &quot;&quot; &#123; log.Printf(&quot;Error: Could not find device name for device id: %s&quot;, k) deviceName = k &#125; log.Printf(&quot;DP Name %s&quot;, deviceName) dp := NewGenericDevicePlugin(deviceName, &quot;/sys/kernel/iommu_groups/&quot;, devs) err := startDevicePlugin(dp) if err != nil &#123; log.Printf(&quot;Error starting %s device plugin: %v&quot;, dp.deviceName, err) &#125; else &#123; devicePlugins = append(devicePlugins, dp) &#125; &#125; //Iterate over vGpuMap to create device plugin for each type of vGPU on the host for k, v := range vGpuMap &#123; devs = nil for _, dev := range v &#123; devs = append(devs, &amp;pluginapi.Device&#123; ID: dev.addr, Health: pluginapi.Healthy, &#125;) &#125; deviceName := getDeviceName(k) if deviceName == &quot;&quot; &#123; deviceName = k &#125; log.Printf(&quot;DP Name %s&quot;, deviceName) dp := NewGenericVGpuDevicePlugin(deviceName, vGpuBasePath, devs) err := startVgpuDevicePlugin(dp) if err != nil &#123; log.Printf(&quot;Error starting %s device plugin: %v&quot;, dp.deviceName, err) &#125; else &#123; vGpuDevicePlugins = append(vGpuDevicePlugins, dp) &#125; &#125; &lt;-stop log.Printf(&quot;Shutting down device plugin controller&quot;) for _, v := range devicePlugins &#123; v.Stop() &#125;&#125; createDevicePlugin的流程图如下： 启动kubevirt-gpu-device-plugin步骤主要有以下几点： 遍历 deviveMap 将所有 device 设备类型下的所有 gpu 标记为 healthy remove 并重建 socket 文件（”/var/lib/kubelet/device-plugins”+”kubevirt-.sock”） 启动 devicePlugin 的 grpc server，对外提供服务 请求kubelet socket链接，进行Regster 启动一个协程对设备进行healthCheck，并监听device-plguin自身socket文件 总结kubevirt-gpu-device-plugin使用前提，是用户使用 vfio-pci 将设备透传至 vm 内，然后，通过读取 vm 内的 pci 设备文件，获取设备的相关信息。iommu,vfio-pci等相关的内容，后续有时间会再补充。 参考资源https://rtoax.blog.csdn.net/article/details/110843839#t11 https://cloud.tencent.com/developer/article/1816469","categories":[{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/categories/device-plugins/"}],"tags":[{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/tags/device-plugins/"},{"name":"gpu","slug":"gpu","permalink":"http://lizhewei91.github.io/tags/gpu/"},{"name":"kubevirt-gpu-device-plugin","slug":"kubevirt-gpu-device-plugin","permalink":"http://lizhewei91.github.io/tags/kubevirt-gpu-device-plugin/"}]},{"title":"NVIDIA/k8s-device-plugin源码分析","slug":"nvidia-device-plugin","date":"2023-01-06T07:43:46.000Z","updated":"2023-01-10T09:08:02.649Z","comments":true,"path":"2023/01/06/46/","link":"","permalink":"http://lizhewei91.github.io/2023/01/06/46/","excerpt":"","text":"device-plugin启动该篇文章基于NVIDIA/k8s-device-plugin: v0.13.0，https://github.com/NVIDIA/k8s-device-plugin/tree/v0.13.0 一切从 main 函数开始作为入口： k8s-device-plugin/cmd/nvidia-device-plugin/main.go#35 func main() &#123; var configFile string c := cli.NewApp() c.Name = &quot;NVIDIA Device Plugin&quot; c.Usage = &quot;NVIDIA device plugin for Kubernetes&quot; c.Version = info.GetVersionString() c.Action = func(ctx *cli.Context) error &#123; return start(ctx, c.Flags) ... &#125; 接下来，调用 start 函数 k8s-device-plugin/cmd/nvidia-device-plugin/main.go#133 func start(c *cli.Context, flags []cli.Flag) error &#123; log.Println(&quot;Starting FS watcher.&quot;) watcher, err := newFSWatcher(pluginapi.DevicePluginPath) if err != nil &#123; return fmt.Errorf(&quot;failed to create FS watcher: %v&quot;, err) &#125; defer watcher.Close() log.Println(&quot;Starting OS watcher.&quot;) sigs := newOSWatcher(syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) var restarting bool var restartTimeout &lt;-chan time.Time var plugins []*NvidiaDevicePluginrestart: // If we are restarting, stop plugins from previous run. if restarting &#123; err := stopPlugins(plugins) if err != nil &#123; return fmt.Errorf(&quot;error stopping plugins from previous run: %v&quot;, err) &#125; &#125; log.Println(&quot;Starting Plugins.&quot;) plugins, restartPlugins, err := startPlugins(c, flags, restarting) if err != nil &#123; return fmt.Errorf(&quot;error starting plugins: %v&quot;, err) &#125; if restartPlugins &#123; log.Printf(&quot;Failed to start one or more plugins. Retrying in 30s...&quot;) restartTimeout = time.After(30 * time.Second) &#125; restarting = true // Start an infinite loop, waiting for several indicators to either log // some messages, trigger a restart of the plugins, or exit the program. for &#123; select &#123; // If the restart timout has expired, then restart the plugins case &lt;-restartTimeout: goto restart // Detect a kubelet restart by watching for a newly created // &#x27;pluginapi.KubeletSocket&#x27; file. When this occurs, restart this loop, // restarting all of the plugins in the process. case event := &lt;-watcher.Events: if event.Name == pluginapi.KubeletSocket &amp;&amp; event.Op&amp;fsnotify.Create == fsnotify.Create &#123; log.Printf(&quot;inotify: %s created, restarting.&quot;, pluginapi.KubeletSocket) goto restart &#125; // Watch for any other fs errors and log them. case err := &lt;-watcher.Errors: log.Printf(&quot;inotify: %s&quot;, err) // Watch for any signals from the OS. On SIGHUP, restart this loop, // restarting all of the plugins in the process. On all other // signals, exit the loop and exit the program. case s := &lt;-sigs: switch s &#123; case syscall.SIGHUP: log.Println(&quot;Received SIGHUP, restarting.&quot;) goto restart default: log.Printf(&quot;Received signal \\&quot;%v\\&quot;, shutting down.&quot;, s) goto exit &#125; &#125; &#125;exit: err = stopPlugins(plugins) if err != nil &#123; return fmt.Errorf(&quot;error stopping plugins: %v&quot;, err) &#125; return nil&#125; Startk8s-device-plugin/cmd/nvidia-device-plugin/server.go#92 K8s-device-plugin的启动流程中，nvidiaDevicePlugin.Start主要有三个步骤，1.启动device-plugin的grpc服务；2.向kubelet注册；3.启动协程对设备checkHealth。 func (plugin *NvidiaDevicePlugin) Start() error &#123; plugin.initialize() err := plugin.Serve() if err != nil &#123; log.Printf(&quot;Could not start device plugin for &#x27;%s&#x27;: %s&quot;, plugin.rm.Resource(), err) plugin.cleanup() return err &#125; log.Printf(&quot;Starting to serve &#x27;%s&#x27; on %s&quot;, plugin.rm.Resource(), plugin.socket) err = plugin.Register() if err != nil &#123; log.Printf(&quot;Could not register device plugin: %s&quot;, err) plugin.Stop() return err &#125; log.Printf(&quot;Registered device plugin for &#x27;%s&#x27; with Kubelet&quot;, plugin.rm.Resource()) go func() &#123; err := plugin.rm.CheckHealth(plugin.stop, plugin.health) if err != nil &#123; log.Printf(&quot;Failed to start health check: %v; continuing with health checks disabled&quot;, err) &#125; &#125;() return nil&#125; Servek8s-device-plugin/cmd/nvidia-device-plugin/server.go#136 k8s-device-plugin 启动 gRPC 服务，对外提供服务 func (plugin *NvidiaDevicePlugin) Serve() error &#123; os.Remove(plugin.socket) sock, err := net.Listen(&quot;unix&quot;, plugin.socket) if err != nil &#123; return err &#125; pluginapi.RegisterDevicePluginServer(plugin.server, plugin) go func() &#123; lastCrashTime := time.Now() restartCount := 0 for &#123; log.Printf(&quot;Starting GRPC server for &#x27;%s&#x27;&quot;, plugin.rm.Resource()) err := plugin.server.Serve(sock) if err == nil &#123; break &#125; log.Printf(&quot;GRPC server for &#x27;%s&#x27; crashed with error: %v&quot;, plugin.rm.Resource(), err) // restart if it has not been too often // i.e. if server has crashed more than 5 times and it didn&#x27;t last more than one hour each time if restartCount &gt; 5 &#123; // quit log.Fatalf(&quot;GRPC server for &#x27;%s&#x27; has repeatedly crashed recently. Quitting&quot;, plugin.rm.Resource()) &#125; timeSinceLastCrash := time.Since(lastCrashTime).Seconds() lastCrashTime = time.Now() if timeSinceLastCrash &gt; 3600 &#123; // it has been one hour since the last crash.. reset the count // to reflect on the frequency restartCount = 1 &#125; else &#123; restartCount++ &#125; &#125; &#125;() // Wait for server to start by launching a blocking connexion conn, err := plugin.dial(plugin.socket, 5*time.Second) if err != nil &#123; return err &#125; conn.Close() return nil&#125; serve主要负责重建nvidia.sock文件，并且注册5个grpc接口到grpc.Server。 Registerk8s-device-plugin/cmd/nvidia-device-plugin/server.go#186 Serve之后，接着进入Register流程，其代码如下： func (plugin *NvidiaDevicePlugin) Register() error &#123; conn, err := plugin.dial(pluginapi.KubeletSocket, 5*time.Second) if err != nil &#123; return err &#125; defer conn.Close() client := pluginapi.NewRegistrationClient(conn) reqt := &amp;pluginapi.RegisterRequest&#123; Version: pluginapi.Version, Endpoint: path.Base(plugin.socket), ResourceName: string(plugin.rm.Resource()), Options: &amp;pluginapi.DevicePluginOptions&#123; GetPreferredAllocationAvailable: true, &#125;, &#125; _, err = client.Register(context.Background(), reqt) if err != nil &#123; return err &#125; return nil&#125; Register的实现流程图如下： 注册的Resource Name是nvidia.com/gpu 注册的Version是v1beta1 CheckHealthk8s-device-plugin/internal/rm/health.go#42 启动协程开始对管理的devices进行健康状态空空，一旦发现有device unhealthy，则发送到NvidiaDevicePlugin的health channel。 func (r *nvmlResourceManager) checkHealth(stop &lt;-chan interface&#123;&#125;, devices Devices, unhealthy chan&lt;- *Device) error &#123; disableHealthChecks := strings.ToLower(os.Getenv(envDisableHealthChecks)) if disableHealthChecks == &quot;all&quot; &#123; disableHealthChecks = allHealthChecks &#125; if strings.Contains(disableHealthChecks, &quot;xids&quot;) &#123; return nil &#125; ret := r.nvml.Init() if ret != nvml.SUCCESS &#123; if *r.config.Flags.FailOnInitError &#123; return fmt.Errorf(&quot;failed to initialize NVML: %v&quot;, ret) &#125; return nil &#125; defer func() &#123; ret := r.nvml.Shutdown() if ret != nvml.SUCCESS &#123; log.Printf(&quot;Error shutting down NVML: %v&quot;, ret) &#125; &#125;() // FIXME: formalize the full list and document it. // http://docs.nvidia.com/deploy/xid-errors/index.html#topic_4 // Application errors: the GPU should still be healthy applicationErrorXids := []uint64&#123; 13, // Graphics Engine Exception 31, // GPU memory page fault 43, // GPU stopped processing 45, // Preemptive cleanup, due to previous errors 68, // Video processor exception &#125; skippedXids := make(map[uint64]bool) for _, id := range applicationErrorXids &#123; skippedXids[id] = true &#125; for _, additionalXid := range getAdditionalXids(disableHealthChecks) &#123; skippedXids[additionalXid] = true &#125; eventSet, ret := r.nvml.EventSetCreate() if ret != nvml.SUCCESS &#123; return fmt.Errorf(&quot;failed to create event set: %v&quot;, ret) &#125; defer eventSet.Free() parentToDeviceMap := make(map[string]*Device) deviceIDToGiMap := make(map[string]int) deviceIDToCiMap := make(map[string]int) eventMask := uint64(nvml.EventTypeXidCriticalError | nvml.EventTypeDoubleBitEccError | nvml.EventTypeSingleBitEccError) for _, d := range devices &#123; uuid, gi, ci, err := r.getDevicePlacement(d) if err != nil &#123; log.Printf(&quot;Warning: could not determine device placement for %v: %v; Marking it unhealthy.&quot;, d.ID, err) unhealthy &lt;- d continue &#125; deviceIDToGiMap[d.ID] = gi deviceIDToCiMap[d.ID] = ci parentToDeviceMap[uuid] = d gpu, ret := r.nvml.DeviceGetHandleByUUID(uuid) if ret != nvml.SUCCESS &#123; log.Printf(&quot;unable to get device handle from UUID: %v; marking it as unhealthy&quot;, ret) unhealthy &lt;- d continue &#125; supportedEvents, ret := gpu.GetSupportedEventTypes() if ret != nvml.SUCCESS &#123; log.Printf(&quot;unabled to determine the supported events for %v: %v; marking it as unhealthy&quot;, d.ID, ret) unhealthy &lt;- d continue &#125; ret = gpu.RegisterEvents(eventMask&amp;supportedEvents, eventSet) if ret == nvml.ERROR_NOT_SUPPORTED &#123; log.Printf(&quot;Warning: Device %v is too old to support healthchecking.&quot;, d.ID) &#125; if ret != nvml.SUCCESS &#123; log.Printf(&quot;Marking device %v as unhealthy: %v&quot;, d.ID, ret) unhealthy &lt;- d &#125; &#125; for &#123; select &#123; case &lt;-stop: return nil default: &#125; e, ret := eventSet.Wait(5000) if ret == nvml.ERROR_TIMEOUT &#123; continue &#125; if ret != nvml.SUCCESS &#123; log.Printf(&quot;Error waiting for event: %v; Marking all devices as unhealthy&quot;, ret) for _, d := range devices &#123; unhealthy &lt;- d &#125; continue &#125; if e.EventType != nvml.EventTypeXidCriticalError &#123; log.Printf(&quot;Skipping non-nvmlEventTypeXidCriticalError event: %+v&quot;, e) continue &#125; if skippedXids[e.EventData] &#123; log.Printf(&quot;Skipping event %+v&quot;, e) continue &#125; log.Printf(&quot;Processing event %+v&quot;, e) eventUUID, ret := e.Device.GetUUID() if ret != nvml.SUCCESS &#123; // If we cannot reliably determine the device UUID, we mark all devices as unhealthy. log.Printf(&quot;Failed to determine uuid for event %v: %v; Marking all devices as unhealthy.&quot;, e, ret) for _, d := range devices &#123; unhealthy &lt;- d &#125; continue &#125; d, exists := parentToDeviceMap[eventUUID] if !exists &#123; log.Printf(&quot;Ignoring event for unexpected device: %v&quot;, eventUUID) continue &#125; if d.IsMigDevice() &amp;&amp; e.GpuInstanceId != 0xFFFFFFFF &amp;&amp; e.ComputeInstanceId != 0xFFFFFFFF &#123; gi := deviceIDToGiMap[d.ID] ci := deviceIDToCiMap[d.ID] if !(uint32(gi) == e.GpuInstanceId &amp;&amp; uint32(ci) == e.ComputeInstanceId) &#123; continue &#125; log.Printf(&quot;Event for mig device %v (gi=%v, ci=%v)&quot;, d.ID, gi, ci) &#125; log.Printf(&quot;XidCriticalError: Xid=%d on Device=%s; marking device as unhealthy.&quot;, e.EventData, d.ID) unhealthy &lt;- d &#125;&#125; checkhealth的主要原理图如下： 需要特别说明healthcheck部分： healthcheck启动协程对管理的devices进行健康状态监控，一旦发现有device unhealthy，则发送到NvidiaDevicePlugin的health channel。device plugin的ListAndWatch会从health channel中获取这些unhealthy devices，并通知到kubelet进行更新。 只监控EventTypeXidCriticalError事件，一旦监控到某个device的这个Event，就认为该device unhealthy。关于EventTypeXidCriticalError的说明，请参考NVIDIA的nvml api文档。 可以通过设置NVIDIA device plugin Pod内的环境变量DP_DISABLE_HEALTHCHECKS为”all”来取消healthcheck。不设置或者设置为其他值都会启动healthcheck，默认部署时不设置。 stopPluginsk8s-device-plugin/cmd/nvidia-device-plugin/main.go#275 func (plugin *NvidiaDevicePlugin) Stop() error &#123; if plugin == nil || plugin.server == nil &#123; return nil &#125; log.Printf(&quot;Stopping to serve &#x27;%s&#x27; on %s&quot;, plugin.rm.Resource(), plugin.socket) plugin.server.Stop() if err := os.Remove(plugin.socket); err != nil &amp;&amp; !os.IsNotExist(err) &#123; return err &#125; plugin.cleanup() return nil&#125; stopPlugins主要做三件事： 停止 device-plugin 的 grpc server 移除 plugin socket 文件 （/var/lib/kubelet/device-plugins/nvidia-gpu.sock） 清空 nvidiaDevicePlugin 相关字段。（plugin.server=nil; plugin.health=nil; plugin.stop=nil） ListAndWatchk8s-device-plugin/cmd/nvidia-device-plugin/server.go#219 func (plugin *NvidiaDevicePlugin) ListAndWatch(e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer) error &#123; s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: plugin.apiDevices()&#125;) for &#123; select &#123; case &lt;-plugin.stop: return nil case d := &lt;-plugin.health: // FIXME: there is no way to recover from the Unhealthy state. d.Health = pluginapi.Unhealthy log.Printf(&quot;&#x27;%s&#x27; device marked unhealthy: %s&quot;, plugin.rm.Resource(), d.ID) s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: plugin.apiDevices()&#125;) &#125; &#125;&#125; listAndWatch 的实现流程图如下： Allocatek8s-device-plugin/cmd/nvidia-device-plugin/server.go#254 allocateRequest的请求结构体如下： type AllocateRequest struct &#123; ContainerRequests []*ContainerAllocateRequest&#125;type ContainerAllocateRequest struct &#123; DevicesIDs []string &#125; allocateResponse的结构体如下： type AllocateResponse struct &#123; ContainerResponses []*ContainerAllocateResponse&#125;type ContainerAllocateResponse struct &#123; Envs map[string]string Mounts []*Mount Devices []*DeviceSpec Annotations map[string]string&#125; Allocate 在容器创建期间调用，这样设备插件可以运行一些特定于设备的操作，并告诉 kubelet 如何令 Device 可在容器中访问的所需执行的具体步骤。 func (plugin *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) &#123; responses := pluginapi.AllocateResponse&#123;&#125; for _, req := range reqs.ContainerRequests &#123; // If the devices being allocated are replicas, then (conditionally) // error out if more than one resource is being allocated. if plugin.config.Sharing.TimeSlicing.FailRequestsGreaterThanOne &amp;&amp; rm.AnnotatedIDs(req.DevicesIDs).AnyHasAnnotations() &#123; if len(req.DevicesIDs) &gt; 1 &#123; return nil, fmt.Errorf(&quot;request for &#x27;%v: %v&#x27; too large: maximum request size for shared resources is 1&quot;, plugin.rm.Resource(), len(req.DevicesIDs)) &#125; &#125; for _, id := range req.DevicesIDs &#123; if !plugin.rm.Devices().Contains(id) &#123; return nil, fmt.Errorf(&quot;invalid allocation request for &#x27;%s&#x27;: unknown device: %s&quot;, plugin.rm.Resource(), id) &#125; &#125; response := pluginapi.ContainerAllocateResponse&#123;&#125; ids := req.DevicesIDs deviceIDs := plugin.deviceIDsFromAnnotatedDeviceIDs(ids) if *plugin.config.Flags.Plugin.DeviceListStrategy == spec.DeviceListStrategyEnvvar &#123; response.Envs = plugin.apiEnvs(plugin.deviceListEnvvar, deviceIDs) &#125; if *plugin.config.Flags.Plugin.DeviceListStrategy == spec.DeviceListStrategyVolumeMounts &#123; response.Envs = plugin.apiEnvs(plugin.deviceListEnvvar, []string&#123;deviceListAsVolumeMountsContainerPathRoot&#125;) response.Mounts = plugin.apiMounts(deviceIDs) &#125; if *plugin.config.Flags.Plugin.PassDeviceSpecs &#123; response.Devices = plugin.apiDeviceSpecs(*plugin.config.Flags.NvidiaDriverRoot, ids) &#125; if *plugin.config.Flags.GDSEnabled &#123; response.Envs[&quot;NVIDIA_GDS&quot;] = &quot;enabled&quot; &#125; if *plugin.config.Flags.MOFEDEnabled &#123; response.Envs[&quot;NVIDIA_MOFED&quot;] = &quot;enabled&quot; &#125; responses.ContainerResponses = append(responses.ContainerResponses, &amp;response) &#125; return &amp;responses, nil&#125; Allocate中会遍历ContainerRequests，将DeviceIDs封装到ContainerAllocateResponse的Envs:NVIDIA_VISIBLE_DEVICES中，格式为：”$&#123;ID_1&#125;,$&#123;ID_2&#125;,...” 除此之外，并没有封装Mounts, Devices, Annotations。 总结NVIDIA/k8s-device-plugin的代码中，依赖于nvidia-docker代码库，存在很多golang调用C库的地方，还需要大家自行到 nvml api文档 中查看相关C函数声明。这篇博客介绍NVIDIA/k8s-device-plugin的代码实现流程，下一篇博客我觉得还有必要对kubelet device plugin manger进行代码分析，如此才能完整的理解整个交互细节。","categories":[{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/categories/device-plugins/"}],"tags":[{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/tags/device-plugins/"},{"name":"nvidia","slug":"nvidia","permalink":"http://lizhewei91.github.io/tags/nvidia/"},{"name":"gpu","slug":"gpu","permalink":"http://lizhewei91.github.io/tags/gpu/"}]},{"title":"浅谈k8s中device-plugins机制","slug":"device-plugins","date":"2022-11-29T02:53:16.000Z","updated":"2023-01-06T07:31:40.008Z","comments":true,"path":"2022/11/29/16/","link":"","permalink":"http://lizhewei91.github.io/2022/11/29/16/","excerpt":"","text":"Extended Resource官方链接：extended-resource-node 特性状态： Kubernetes v1.9 [stable] 可以用一句话来概括这个特性：通过向apiserver发送一个 patch node 的请求，为这个node增加一个自定义的资源类型，用于以该资源的配额统计和相应的QoS的配置。 为节点增加扩展资源为在一个节点上发布一种新的扩展资源，需要发送一个 HTTP PATCH 请求到 Kubernetes API server。 例如：假设你的一个节点上带有四个 dongle 资源。 下面是一个 PATCH 请求的示例，该请求为你的节点发布四个 dongle 资源。 PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1Accept: application/jsonContent-Type: application/json-patch+jsonHost: k8s-master:8080[ &#123; &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;, &quot;value&quot;: &quot;4&quot; &#125;] 注意：Kubernetes 不需要了解 dongle 资源的含义和用途。 前面的 PATCH 请求告诉 Kubernetes 你的节点拥有四个你称之为 dongle 的东西。 启动一个代理（proxy），以便你可以很容易地向 Kubernetes API server 发送请求： kubectl proxy 在另一个命令窗口中，发送 HTTP PATCH 请求。 用你的节点名称替换 &lt;your-node-name&gt;： curl --header &quot;Content-Type: application/json-patch+json&quot; \\ --request PATCH \\ --data &#x27;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;, &quot;value&quot;: &quot;4&quot;&#125;]&#x27; \\ http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status 说明： 在前面的请求中，~1 为 patch 路径中 “/” 符号的编码。 JSON-Patch 中的操作路径值被解析为 JSON 指针。 kubectl describe node &lt;your-node-name&gt; 清理扩展资源这里是一个从节点移除 dongle 资源发布的 PATCH 请求。 PATCH /api/v1/nodes/&lt;your-node-name&gt;/status HTTP/1.1Accept: application/jsonContent-Type: application/json-patch+jsonHost: k8s-master:8080[ &#123; &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;, &#125;] 启动一个代理，以便你可以很容易地向 Kubernetes API 服务器发送请求： kubectl proxy 在另一个命令窗口中，发送 HTTP PATCH 请求。用你的节点名称替换 &lt;your-node-name&gt;： curl --header &quot;Content-Type: application/json-patch+json&quot; \\--request PATCH \\--data &#x27;[&#123;&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/status/capacity/example.com~1dongle&quot;&#125;]&#x27; \\http://localhost:8001/api/v1/nodes/&lt;your-node-name&gt;/status 验证 dongle 资源的发布已经被移除： kubectl describe node &lt;your-node-name&gt; | grep dongle (你应该看不到任何输出) 扩展资源是 kubernetes.io 域名之外的标准资源名称。 它们使得集群管理员能够颁布非 Kubernetes 内置资源，而用户可以使用他们。 使用扩展资源需要两个步骤。首先，集群管理员必须颁布扩展资源。 其次，用户必须在 Pod 中请求扩展资源。 Device Plugins官方链接：device-plugins 特性状态： Kubernetes v1.26 [stable] Kubernetes 提供了一个 设备插件框架， 你可以用它来将系统硬件资源发布到 Kubelet。 供应商可以实现设备插件，由你手动部署或作为 DaemonSet 来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、 InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。 设备插件框架 设备插件的常规工作流程包括以下几个步骤： 初始化。在这个阶段，设备插件将执行供应商特定的初始化和设置， 以确保设备处于就绪状态。 插件使用主机路径 /var/lib/kubelet/device-plugins/ 下的 Unix 套接字启动一个 gRPC 服务，如：/var/lib/kubelet/device-plugins/nvidia-gpu.sock，该服务实现以下接口： service DevicePlugin &#123; // GetDevicePluginOptions 返回与设备管理器沟通的选项。 rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) &#123;&#125; // ListAndWatch 返回 Device 列表构成的数据流。 // 当 Device 状态发生变化或者 Device 消失时，ListAndWatch // 会返回新的列表。 rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) &#123;&#125; // Allocate 在容器创建期间调用，这样设备插件可以运行一些特定于设备的操作， // 并告诉 kubelet 如何令 Device 可在容器中访问的所需执行的具体步骤 rpc Allocate(AllocateRequest) returns (AllocateResponse) &#123;&#125; // GetPreferredAllocation 从一组可用的设备中返回一些优选的设备用来分配， // 所返回的优选分配结果不一定会是设备管理器的最终分配方案。 // 此接口的设计仅是为了让设备管理器能够在可能的情况下做出更有意义的决定。 rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) &#123;&#125; // PreStartContainer 在设备插件注册阶段根据需要被调用，调用发生在容器启动之前。 // 在将设备提供给容器使用之前，设备插件可以运行一些诸如重置设备之类的特定于 // 具体设备的操作， rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) &#123;&#125;&#125; 插件通过 Unix socket 在主机路径 /var/lib/kubelet/device-plugins/kubelet.sock 处向 kubelet 注册自身。 成功注册自身后，设备插件将以服务模式运行，在此期间，它将持续监控设备运行状况， 并在设备状态发生任何变化时向 kubelet 报告。它还负责响应 Allocate gRPC 请求。 在 Allocate 期间，设备插件可能还会做一些设备特定的准备；例如 GPU 清理或 QRNG 初始化。 如果操作成功，则设备插件将返回 AllocateResponse，其中包含用于访问被分配的设备容器运行时的配置。 kubelet 将此信息传递到容器运行时。 处理 kubelet 重启 设备插件应能监测到 kubelet 重启，并且向新的 kubelet 实例来重新注册自己。 新的 kubelet 实例启动时会删除 /var/lib/kubelet/device-plugins 下所有已经存在的 Unix 套接字。 设备插件需要能够监控到它的 Unix 套接字被删除，并且当发生此类事件时重新注册自己。","categories":[{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/categories/device-plugins/"}],"tags":[{"name":"kubernetnes","slug":"kubernetnes","permalink":"http://lizhewei91.github.io/tags/kubernetnes/"},{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/tags/device-plugins/"}]},{"title":"hexo+typora图片插入","slug":"hexo-typora-photos","date":"2022-11-28T14:46:29.000Z","updated":"2022-11-28T14:52:49.556Z","comments":true,"path":"2022/11/28/29/","link":"","permalink":"http://lizhewei91.github.io/2022/11/28/29/","excerpt":"","text":"Typora设置。点击文件-&gt;偏好设置-&gt;图像，配置插入图片问复制到指定路径，底下三个勾都选，如下图所示。 Hexo配置。在根目录下，进入bash命令框，输入npm install hexo-image-link --save安装插件。 更改根目录下_config.yml配置，找到post_asset_folder，改为true。 写文章前，在根目录bash命令框中输入hexo new 文章题目，可以自动在post文件夹中生成文章题目.md与文章题目图片存储目录了。 这样就配置成功了。写文章之前，现在你可以把图片像word编辑一样拖动到typora直接预览，该过程不需要输入什么图片插入指令呀存储路径呀啥的，也不需要专门去对图片进行转存；直接网页端预览，图片可以正常显示。","categories":[{"name":"hexo","slug":"hexo","permalink":"http://lizhewei91.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://lizhewei91.github.io/tags/hexo/"}]},{"title":"性能剖析大杀器 pprof","slug":"golang-pprof","date":"2022-11-28T12:28:56.000Z","updated":"2023-06-01T07:36:49.829Z","comments":true,"path":"2022/11/28/56/","link":"","permalink":"http://lizhewei91.github.io/2022/11/28/56/","excerpt":"","text":"pprof在 Go 语言中，PProf 是用于可视化和分析性能分析数据的工具，pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）。 而刚刚提到的 profile.proto 是一个 Protobuf v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式。 可以做什么 CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置。 Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏。 Block Profiling：阻塞分析，记录Goroutine阻塞等待同步（包括定时器通道）的位置，默认不开启，需要调用 runtime.SetBlockProfileRate进行设置。 Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况，默认不开启，需要调用 runtime.SetMutexProfileFraction 进行设置。 Goroutine Profiling：Goroutine 分析，可以对当前应用程序正在运行的 Goroutine 进行堆栈跟踪和分析。 其中像是 Goroutine Profiling 这项功能会在实际排查中会经常用到。因为很多问题出现时的表象就是 Goroutine 暴增，而这时候我们要做的事情之一就是查看应用程序中的 Goroutine 正在做什么事情，因为什么阻塞了，然后再进行下一步。 采样方式 runtime/pprof：采集程序（非 Server）的指定区块的运行数据进行分析。 net/http/pprof：基于HTTP Server运行，并且可以采集运行时数据进行分析。 go test：通过运行测试用例，并指定所需标识来进行采集。 使用模式 Report generation：报告生成。 Interactive terminal use：交互式终端使用。 Web interface：Web 界面。 服务型应用如果你的应用程序是一直运行的，比如 web 应用，那么可以使用net/http/pprof库，它能够在提供 HTTP 服务进行分析。 如果使用了默认的 http.DefaultServeMux（通常是代码直接使用 http.ListenAndServe(“0.0.0.0:8080”, nil)），只需要在你的web server端代码中按如下方式导入net/http/pprof import _ &quot;net/http/pprof&quot; 为什么要初始化net/http/pprof在我们的例子中，你会发现我们在引用上对 net/http/pprof包进行了默认的初始化（也就是 _），如果你曾经漏了，或者没加，你会发现压根调用不了 pprof 的相关接口，这是为什么呢，我们一起看看下面该包的初始化方法，如下： func init() &#123; http.HandleFunc(&quot;/debug/pprof/&quot;, Index) http.HandleFunc(&quot;/debug/pprof/cmdline&quot;, Cmdline) http.HandleFunc(&quot;/debug/pprof/profile&quot;, Profile) http.HandleFunc(&quot;/debug/pprof/symbol&quot;, Symbol) http.HandleFunc(&quot;/debug/pprof/trace&quot;, Trace)&#125; 实际上 net/http/pprof会在初始化函数中对标准库中net/http所默认提供的 DefaultServeMux 进行路由注册，源码如下： var DefaultServeMux = &amp;defaultServeMuxvar defaultServeMux ServeMuxfunc HandleFunc(pattern string, handler func(ResponseWriter, *Request)) &#123; DefaultServeMux.HandleFunc(pattern, handler)&#125; 而我们在例子中使用的 HTTP Server，也是使用的标准库中默认提供的，因此便完美的结合在了一起，这也恰好也是最小示例的模式。 这时候你可能会注意到另外一个问题，那就是我们的实际项目中，都是有相对独立的 ServeMux 的，这时候我们只要仿照着将 pprof 对应的路由注册进去就好了，如下： mux := http.NewServeMux()mux.HandleFunc(&quot;/debug/pprof/&quot;, pprof.Index)mux.HandleFunc(&quot;/debug/pprof/cmdline&quot;, pprof.Cmdline)mux.HandleFunc(&quot;/debug/pprof/profile&quot;, pprof.Profile)mux.HandleFunc(&quot;/debug/pprof/symbol&quot;, pprof.Symbol)mux.HandleFunc(&quot;/debug/pprof/trace&quot;, pprof.Trace) 如果你使用的是gin框架，那么推荐使用 github.com/gin-contrib/pprof，在代码中通过以下命令注册 pprof 相关路由。 pprof.Register(router) 一个简单的例子package mainimport ( &quot;log&quot; &quot;net/http&quot; _ &quot;net/http/pprof&quot; &quot;time&quot;)var datas []stringfunc main() &#123; str := &quot;hello,world&quot; go func() &#123; for &#123; log.Printf(&quot;len:%d\\n&quot;, Add(str)) time.Sleep(10 * time.Millisecond) &#125; &#125;() _ = http.ListenAndServe(&quot;0.0.0.0:8080&quot;, nil)&#125;func Add(str string) int &#123; datas = append(datas, str) return len(datas)&#125; 接下来我们运行这个程序，访问 http://127.0.0.1:8080/debug/pprof/ 地址，检查是否正常响应。 通过浏览器访问第一种方式，我们可以直接通过浏览器，进行查看，那么在第一步我们可以先查看总览页面，也就是访问 http://127.0.0.1:8080/debug/pprof/，如下： /debug/pprof/Types of profiles available:Count Profile3 allocs0 block0 cmdline5 goroutine3 heap0 mutex0 profile8 threadcreate0 tracefull goroutine stack dump allocs：查看过去所有内存分配的样本，访问路径为$HOST/debug/pprof/allocs。 block：查看导致阻塞同步的堆栈跟踪，访问路径为$HOST/debug/pprof/block。 cmdline：当前程序的命令行的完整调用路径，访问路径为$HOST/debug/pprof/cmdline。 goroutine：查看当前所有运行的 goroutines 堆栈跟踪，访问路径为$HOST/debug/pprof/goroutine。 heap：查看活动对象的内存分配情况， 访问路径为$HOST/debug/pprof/heap。 mutex：查看导致互斥锁的竞争持有者的堆栈跟踪，访问路径为$HOST/debug/pprof/mutex。 profile：默认进行 30s 的 CPU Profiling，得到一个分析用的 profile 文件，访问路径为$HOST/debug/pprof/profile。 threadcreate：查看创建新OS线程的堆栈跟踪，访问路径为$HOST/debug/pprof/threadcreate。 如果在相应的路径上加“?debug=1”，则可以直接在浏览器中访问，如图所示： 若不新增debug参数，则会直接下载对应的profile文件。 注意：debug的访问方式具有时效性，在实际场景中，我们通常将profile文件保存下来，便于二次分析。 通过交互式终端使用CPU Profiling第二种方式，我们可以直接通过命令行，来完成对正在运行的应用程序 pprof 的抓取和分析。 $ go tool pprof http://localhost:8080/debug/pprof/profile?seconds=60Fetching profile over HTTP from http://localhost:6060/debug/pprof/profile?seconds=60Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.002.pb.gzType: cpuDuration: 1mins, Total samples = 37.25s (61.97%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) 执行该命令后，需等待 60 秒（可调整 seconds 的值），pprof 会进行 CPU Profiling，结束后将默认进入 pprof 的命令行交互式模式，可以对分析的结果进行查看或导出。另外如果你所启动的 HTTP Server 是 TLS 的方式，那么在调用go tool pprof 时，需要将调用路径改为：go tool pprof https+insecure://localhost:8080/debug/pprof/profile\\?seconds\\=60。 (pprof) top10Showing nodes accounting for 1.38s, 100% of 1.38s totalShowing top 10 nodes out of 50 flat flat% sum% cum cum% 0.60s 43.48% 43.48% 0.80s 57.97% runtime.kevent 0.31s 22.46% 65.94% 0.31s 22.46% runtime.libcCall 0.21s 15.22% 81.16% 0.22s 15.94% syscall.syscall 0.15s 10.87% 92.03% 0.26s 18.84% runtime.pthread_cond_wait 0.04s 2.90% 94.93% 0.04s 2.90% runtime.pthread_cond_signal 0.02s 1.45% 96.38% 0.02s 1.45% runtime.walltime 0.02s 1.45% 97.83% 0.02s 1.45% runtime.write1 0.01s 0.72% 98.55% 0.01s 0.72% log.itoa 0.01s 0.72% 99.28% 0.01s 0.72% runtime.(*mcache).prepareForSweep 0.01s 0.72% 100% 0.01s 0.72% runtime.memmove flat：函数自身的运行耗时。 flat%：函数自身在 CPU 运行耗时总比例。 sum%：函数自身累积使用 CPU 总比例。 cum：函数自身及其调用函数的运行总耗时。 cum%：函数自身及其调用函数的运行耗时总比例。 Name：函数名。 在大多数的情况下，我们可以通过这五列得出一个应用程序的运行情况，知道当前是什么函数，正在做什么事情，占用了多少资源，谁又是占用资源的大头，以此来得到一个初步的分析方向。 另外在交互命令行中，pprof 还支持了大量的其它命令，具体可执行 pprof help 查看帮助说明。 Heap Profiling$ go tool pprof http://localhost:8080/debug/pprof/heapFetching profile over HTTP from http://localhost:8080/debug/pprof/heapSaved profile in /Users/lizhewei/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.001.pb.gzType: inuse_spaceTime: Dec 15, 2021 at 4:56pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) 执行该命令后，能够很快的拉取到其结果，因为它不需要像 CPU Profiling 做采样等待，这里需要注意的一点是 Type 这一个选项，你可以看到它默认显示的是 inuse_space，实际上可以针对多种内存概况进行分析，常用的类别如下： 一共有四种类型： inuse_space：分析应用程序的常驻内存占用情况。 alloc_objects：分析应用程序的内存临时分配情况。 inuse_objects：查看每个函数所分别的对象数量。 alloc_space：查看分配的内存空间大小。 inuse_space：分析应用程序的常驻内存占用情况。 $ go tool pprof -inuse_space http://localhost:8080/debug/pprof/heapFetching profile over HTTP from http://localhost:8080/debug/pprof/heapSaved profile in /Users/lizhewei/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gzType: inuse_spaceTime: Dec 15, 2021 at 4:59pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 4130.49kB, 100% of 4130.49kB totalShowing top 10 nodes out of 20 flat flat% sum% cum cum% 1567.04kB 37.94% 37.94% 1567.04kB 37.94% main.Add (inline) 1537.69kB 37.23% 75.17% 1537.69kB 37.23% runtime.allocm 513.56kB 12.43% 87.60% 513.56kB 12.43% regexp/syntax.init 512.20kB 12.40% 100% 512.20kB 12.40% runtime.malg 0 0% 100% 1567.04kB 37.94% main.main.func1 alloc_objects：分析应用程序的内存临时分配情况。 $ go tool pprof -alloc_objects http://localhost:8080/debug/pprof/heapFetching profile over HTTP from http://localhost:8080/debug/pprof/heapSaved profile in /Users/lizhewei/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.003.pb.gzType: alloc_objectsTime: Dec 15, 2021 at 5:01pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 311313, 99.07% of 314251 totalDropped 39 nodes (cum &lt;= 1571) flat flat% sum% cum cum% 163842 52.14% 52.14% 311313 99.07% main.main.func1 131074 41.71% 93.85% 131074 41.71% fmt.Sprintf 16397 5.22% 99.07% 16397 5.22% main.Add (inline) 0 0% 99.07% 131074 41.71% log.Printf 另外还有 inuse_objects 和 alloc_space 类别，分别对应查看每个函数所分别的对象数量和查看分配的内存空间大小，具体可根据情况选用。 Goroutine Profiling$ go tool pprof http://localhost:8080/debug/pprof/goroutineFetching profile over HTTP from http://localhost:8080/debug/pprof/goroutineSaved profile in /Users/lizhewei/pprof/pprof.goroutine.001.pb.gzType: goroutineTime: Dec 15, 2021 at 5:04pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) 在查看 goroutine 时，我们可以使用 traces 命令，这个命令会打印出对应的所有调用栈，以及指标信息，可以让我们很便捷的查看到整个调用链路有什么，分别在哪里使用了多少个 goroutine，并且能够通过分析查看到谁才是真正的调用方，输出结果如下： (pprof) tracesType: goroutine-----------+------------------------------------------------------- 2 runtime.gopark runtime.netpollblock internal/poll.runtime_pollWait ...-----------+------------------------------------------------------- 1 runtime.gopark runtime.netpollblock ... net/http.ListenAndServe main.main runtime.main 在调用栈上来讲，其展示顺序是自下而上的，也就是 runtime.main 方法调用了 main.main 方法，main.main 方法又调用了 net/http.ListenAndServe 方法，这里对应的也就是我们所使用的示例代码了，排查起来会非常方便。 每个调用堆栈信息用 ----------- 分割，函数方法前的就是指标数据，像 Goroutine Profiling 展示是就是该方法占用的 goroutine 的数量。而 Heap Profiling 展示的就是占用的内存大小，如下： $ go tool pprof http://localhost:8080/debug/pprof/heap...Type: inuse_spaceEntering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) tracesType: inuse_space-----------+------------------------------------------------------- bytes: 13.55MB 13.55MB main.Add main.main.func1-----------+------------------------------------------------------- Mutex Profiling怎么样的情况下会造成阻塞呢，一般有如下方式：调用 chan（通道）、调用 sync.Mutex （同步锁）、调用 time.Sleep() 等等。那么为了验证互斥锁的竞争持有者的堆栈跟踪，我们可以根据以上的 sync.Mutex 方式，来调整先前的示例代码，代码如下： func init() &#123; runtime.SetMutexProfileFraction(1)&#125;func main() &#123; var m sync.Mutex var datas = make(map[int]struct&#123;&#125;) for i := 0; i &lt; 999; i++ &#123; go func(i int) &#123; m.Lock() defer m.Unlock() datas[i] = struct&#123;&#125;&#123;&#125; &#125;(i) &#125; _ = http.ListenAndServe(&quot;:6061&quot;, nil)&#125; 需要特别注意的是 runtime.SetMutexProfileFraction 语句，如果未来希望进行互斥锁的采集，那么需要通过调用该方法来设置采集频率，若不设置或没有设置大于 0 的数值，默认是不进行采集的。 接下来我们进行调用 go tool pprof 进行分析，如下： $ go tool pprof http://localhost:8081/debug/pprof/mutexFetching profile over HTTP from http://localhost:8081/debug/pprof/mutexSaved profile in /Users/lizhewei/pprof/pprof.contentions.delay.002.pb.gzType: delayTime: Dec 15, 2021 at 5:18pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) 我们查看调用 top 命令，查看互斥量的排名： (pprof) topShowing nodes accounting for 1.77ms, 100% of 1.77ms total flat flat% sum% cum cum% 1.77ms 100% 100% 1.77ms 100% sync.(*Mutex).Unlock 0 0% 100% 1.77ms 100% main.main.func1 接下来我们可以调用 list 命令，看到指定函数的代码情况（包含特定的指标信息，例如：耗时），若函数名不明确，默认会对函数名进行模糊匹配，如下： (pprof) list mainTotal: 1.77msROUTINE ======================== main.main.func1 in /Volumes/D/go/src/github.com/lizw91/pprof/main.go 0 1.77ms (flat, cum) 100% of Total . . 17: for i := 0; i &lt; 1000; i++ &#123; . . 18: go func(i int) &#123; . . 19: m.Lock() . . 20: defer m.Unlock() . . 21: datas[i] = struct&#123;&#125;&#123;&#125; . 1.77ms 22: &#125;(i) . . 23: &#125; . . 24: . . 25: _ = http.ListenAndServe(&quot;:8081&quot;, nil) . . 26:&#125; 我们可以在输出的分析中比较准确的看到引起互斥锁的函数在哪里，锁开销在哪里，在本例中是第 22 行。 Block Profiling与 Mutex 的 runtime.SetMutexProfileFraction 相似，Block 也需要调用 runtime.SetBlockProfileRate() 进行采集量的设置，否则默认关闭，若设置的值小于等于 0 也会认为是关闭。 与上小节 Mutex 相比，主体代码不变，仅是新增 runtime.SetBlockProfileRate()的调用，如下： 示例代码 package mainimport ( &quot;net/http&quot; _ &quot;net/http/pprof&quot; &quot;runtime&quot; &quot;sync&quot;)func init() &#123; runtime.SetBlockProfileRate(1)&#125;func main() &#123; var m sync.Mutex var datas = make(map[int]struct&#123;&#125;) for i := 0; i &lt; 1000; i++ &#123; go func(i int) &#123; m.Lock() defer m.Unlock() datas[i] = struct&#123;&#125;&#123;&#125; &#125;(i) &#125; _ = http.ListenAndServe(&quot;:8081&quot;, nil)&#125; 我们查看调用 top 命令，查看阻塞情况的排名： $ go tool pprof http://localhost:8081/debug/pprof/blockFetching profile over HTTP from http://localhost:8081/debug/pprof/blockSaved profile in /Users/lizhewei/pprof/pprof.contentions.delay.003.pb.gzType: delayTime: Dec 15, 2021 at 5:21pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 48.97ms, 100% of 48.97ms total flat flat% sum% cum cum% 48.97ms 100% 100% 48.97ms 100% sync.(*Mutex).Lock (inline) 0 0% 100% 48.97ms 100% main.main.func1 同样的，我们也可以调用 list 命令查看具体的阻塞情况，执行方式和排查模式与先前概述的一致。 (pprof) list mainTotal: 48.97msROUTINE ======================== main.main.func1 in /Volumes/D/go/src/github.com/lizw91/pprof/main.go 0 48.97ms (flat, cum) 100% of Total . . 14:func main() &#123; . . 15: var m sync.Mutex . . 16: var datas = make(map[int]struct&#123;&#125;) . . 17: for i := 0; i &lt; 1000; i++ &#123; . . 18: go func(i int) &#123; . 48.97ms 19: m.Lock() . . 20: defer m.Unlock() . . 21: datas[i] = struct&#123;&#125;&#123;&#125; . . 22: &#125;(i) . . 23: &#125; . . 24: 查看可视化界面接下来我们继续使用前面的示例程序，将其重新运行起来，然后在其它窗口执行下述命令： // 获取 cpu 指标$ wget -O cpu.profile http://127.0.0.1:8080/debug/pprof/profile?seconds=30// 获取 heap 指标 $ wget -O mem.profile http://127.0.0.1:8080/debug/pprof/profile?seconds=30 默认需要等待 30 秒，执行完毕后可在当前目录下发现采集的文件 cpu.profile，针对可视化界面我们有两种方式可进行下一步分析： 方法一（推荐）：该命令将在所指定的端口号运行一个 pprof 的分析用的站点 $ go tool pprof -http=:8081 cpu.profile 方法二：通过 web 命令将以 svg 的文件格式写入图形，然后在 Web 浏览器中将其打开。 $ go tool pprof cpu.profileType: cpuTime: Feb 1, 2020 at 12:09pm (CST)Duration: 30s, Total samples = 60ms ( 0.2%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) web 如果出现错误提示 Could not execute dot; may need to install graphviz.，那么意味着你需要安装 graphviz组件。 另外方法一所运行的站点，实际上包含了方法二的内容（svg图片），并且更灵活，因此非特殊情况，我们会直接使用方法一的方式运行站点来做观察和分析。 通过 pprof 所提供的可视化界面，我们能够更方便、更直观的看到 Go 应用程序的调用链、使用情况等。 另外在 View 菜单栏中，PProf 还支持多种分析方式的切换，如下： view 菜单栏 接下来我们将基于 CPU Profiling 所抓取的 Profile 进行一一介绍，而其它 Profile 类型的分析模式也是互通的，只要我们了解了一种，其余的也就会了。 Top flat：函数自身的运行耗时。 flat%：函数自身在 CPU 运行耗时总比例。 sum%：函数自身累积使用 CPU 总比例。 cum：函数自身及其调用函数的运行总耗时。 cum%：函数自身及其调用函数的运行耗时总比例。 Name：函数名。 Graph 该视图展示的为整体的函数调用流程，框越大、线越粗、框颜色越鲜艳（红色）就代表它占用的时间越久，开销越大。相反若框颜色越淡，越小则代表在整体的函数调用流程中，它的开销是相对较小的。因此我们可以用此视图去分析谁才是开销大头，它又是因为什么调用流程而被调用的。 Peek peek 栏目，此视图相较于 Top 视图，增加了所属的上下文信息的展示，也就是函数的输出调用者/被调用者。 Source source 栏目，该视图主要是增加了面向源代码的追踪和分析，可以看到其开销主要消耗在哪里。 flame graph Flame Graph（火焰图）它是可动态的，调用顺序由上到下（A -&gt; B -&gt; C -&gt; D），每一块代表一个函数、颜色越鲜艳（红）、区块越大代表占用 CPU 的时间更长。同时它也支持点击块深入进行分析。 我们选择页面上的 main.main.func1 区块，将会进入到其属下的下一层级，如下： 进一步查看 flame graph，这样子我们就可以根据不同函数的多维度层级进行分析，能够更好的观察其流转并发现问题。 工具型应用如果你的应用程序是运行一段时间就结束退出类型。那么最好的办法是在应用退出的时候把 profiling 的报告保存到文件中，进行分析。对于这种情况，可以使用runtime/pprof库。 首先在代码中导入runtime/pprof工具： import &quot;runtime/pprof&quot; CPU性能分析开启CPU性能分析： pprof.StartCPUProfile(w io.Writer) 停止CPU性能分析： pprof.StopCPUProfile() 应用执行结束后，就会生成一个文件，保存了我们的 CPU profiling 数据。得到采样数据之后，使用 go tool pprof工具进行 CPU 性能分析。 内存性能优化记录程序的堆栈信息 pprof.WriteHeapProfile(w io.Writer) go tool pprof默认是使用-inuse_space 进行统计，还可以使用 -inuse-objects 查看分配对象的数量。 示例// runtime_pprof/main.gopackage mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;os&quot; &quot;runtime/pprof&quot; &quot;time&quot;)// 一段有问题的代码func logicCode() &#123; var c chan int for &#123; select &#123; case v := &lt;-c: fmt.Printf(&quot;recv from chan, value:%v\\n&quot;, v) default: &#125; &#125;&#125;func main() &#123; var isCPUPprof bool var isMemPprof bool flag.BoolVar(&amp;isCPUPprof, &quot;cpu&quot;, false, &quot;turn cpu pprof on&quot;) flag.BoolVar(&amp;isMemPprof, &quot;mem&quot;, false, &quot;turn mem pprof on&quot;) flag.Parse() if isCPUPprof &#123; file, err := os.Create(&quot;./cpu.pprof&quot;) if err != nil &#123; fmt.Printf(&quot;create cpu pprof failed, err:%v\\n&quot;, err) return &#125; pprof.StartCPUProfile(file) defer pprof.StopCPUProfile() &#125; for i := 0; i &lt; 8; i++ &#123; go logicCode() &#125; time.Sleep(20 * time.Second) if isMemPprof &#123; file, err := os.Create(&quot;./mem.pprof&quot;) if err != nil &#123; fmt.Printf(&quot;create mem pprof failed, err:%v\\n&quot;, err) return &#125; pprof.WriteHeapProfile(file) file.Close() &#125;&#125; 通过flag我们可以在命令行控制是否开启 CPU和 Mem 的性能分析。 将上面的代码保存并编译成runtime_pprof 可执行文件，执行时加上 -cpu 命令行参数如下： go run main.go -cpu 等待30秒后会在当前目录下生成一个 cpu.pprof 文件。然后，执行go tool pprof命令就可以查看 $ go tool pprof cpu.pprofType: cpuTime: Dec 15, 2021 at 8:25pm (CST)Duration: 20.19s, Total samples = 119.64s (592.65%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 119.62s, 100% of 119.64s totalDropped 2 nodes (cum &lt;= 0.60s) flat flat% sum% cum cum% 52.65s 44.01% 44.01% 52.65s 44.01% runtime.chanrecv 51.04s 42.66% 86.67% 103.69s 86.67% runtime.selectnbrecv 15.93s 13.31% 100% 119.63s 100% main.logicCode 压测工具wrk推荐使用： https://github.com/wg/wrk https://github.com/adjust/go-wrk 使用wrk进行压测: go-wrk -n 50000 http://127.0.0.1:8080/book/list 在上面压测进行的同时，打开另一个终端执行: $ go tool pprof http://localhost:8080/debug/pprof/profile?seconds=60 pprof与性能测试结合go test 命令有两个参数和 pprof 相关，它们分别指定生成的 CPU 和 Memory profiling 保存的文件： -cpuprofile：cpu profiling 数据要保存的文件地址 -memprofile：memory profiling 数据要报文的文件地址 我们还可以选择将pprof与性能测试相结合，比如： 比如下面执行测试的同时，也会执行 CPU profiling，并把结果保存在 cpu.prof 文件中： go test -bench . -cpuprofile=cpu.profile 比如下面执行测试的同时，也会执行 Mem profiling，并把结果保存在 cpu.prof 文件中： go test -bench . -memprofile=./mem.profile 注意： 获取的 Profiling 数据是动态的，要想获得有效的数据，请保证应用处于较大的负载（比如正在生成中运行的服务，或者通过其他工具模拟访问压力）。否则如果应用处于空闲状态，得到的结果可能没有任何意义。所以，Profiling 一般和性能测试一起使用","categories":[{"name":"pprof","slug":"pprof","permalink":"http://lizhewei91.github.io/categories/pprof/"}],"tags":[{"name":"pprof","slug":"pprof","permalink":"http://lizhewei91.github.io/tags/pprof/"},{"name":"性能分析","slug":"性能分析","permalink":"http://lizhewei91.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"name":"golang","slug":"golang","permalink":"http://lizhewei91.github.io/tags/golang/"}]}],"categories":[{"name":"deploy-k8s","slug":"deploy-k8s","permalink":"http://lizhewei91.github.io/categories/deploy-k8s/"},{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/categories/kubelet/"},{"name":"buildx","slug":"buildx","permalink":"http://lizhewei91.github.io/categories/buildx/"},{"name":"reloader","slug":"reloader","permalink":"http://lizhewei91.github.io/categories/reloader/"},{"name":"krew","slug":"krew","permalink":"http://lizhewei91.github.io/categories/krew/"},{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/categories/device-plugins/"},{"name":"hexo","slug":"hexo","permalink":"http://lizhewei91.github.io/categories/hexo/"},{"name":"pprof","slug":"pprof","permalink":"http://lizhewei91.github.io/categories/pprof/"}],"tags":[{"name":"kubeadm","slug":"kubeadm","permalink":"http://lizhewei91.github.io/tags/kubeadm/"},{"name":"deploy-k8s","slug":"deploy-k8s","permalink":"http://lizhewei91.github.io/tags/deploy-k8s/"},{"name":"kubelet","slug":"kubelet","permalink":"http://lizhewei91.github.io/tags/kubelet/"},{"name":"allocatable","slug":"allocatable","permalink":"http://lizhewei91.github.io/tags/allocatable/"},{"name":"cri","slug":"cri","permalink":"http://lizhewei91.github.io/tags/cri/"},{"name":"volume-manager","slug":"volume-manager","permalink":"http://lizhewei91.github.io/tags/volume-manager/"},{"name":"buildx","slug":"buildx","permalink":"http://lizhewei91.github.io/tags/buildx/"},{"name":"manifest","slug":"manifest","permalink":"http://lizhewei91.github.io/tags/manifest/"},{"name":"reloader","slug":"reloader","permalink":"http://lizhewei91.github.io/tags/reloader/"},{"name":"configMap","slug":"configMap","permalink":"http://lizhewei91.github.io/tags/configMap/"},{"name":"secret","slug":"secret","permalink":"http://lizhewei91.github.io/tags/secret/"},{"name":"kubectl-plugins","slug":"kubectl-plugins","permalink":"http://lizhewei91.github.io/tags/kubectl-plugins/"},{"name":"krew","slug":"krew","permalink":"http://lizhewei91.github.io/tags/krew/"},{"name":"device-manager","slug":"device-manager","permalink":"http://lizhewei91.github.io/tags/device-manager/"},{"name":"device-plugins","slug":"device-plugins","permalink":"http://lizhewei91.github.io/tags/device-plugins/"},{"name":"gpu","slug":"gpu","permalink":"http://lizhewei91.github.io/tags/gpu/"},{"name":"kubevirt-gpu-device-plugin","slug":"kubevirt-gpu-device-plugin","permalink":"http://lizhewei91.github.io/tags/kubevirt-gpu-device-plugin/"},{"name":"nvidia","slug":"nvidia","permalink":"http://lizhewei91.github.io/tags/nvidia/"},{"name":"kubernetnes","slug":"kubernetnes","permalink":"http://lizhewei91.github.io/tags/kubernetnes/"},{"name":"hexo","slug":"hexo","permalink":"http://lizhewei91.github.io/tags/hexo/"},{"name":"pprof","slug":"pprof","permalink":"http://lizhewei91.github.io/tags/pprof/"},{"name":"性能分析","slug":"性能分析","permalink":"http://lizhewei91.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"name":"golang","slug":"golang","permalink":"http://lizhewei91.github.io/tags/golang/"}]}